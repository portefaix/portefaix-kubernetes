# SPDX-FileCopyrightText: Copyright (C) Nicolas Lamirault <nicolas.lamirault@gmail.com>
# SPDX-License-Identifier: Apache-2.0

---
k3s:
  enabled: false

ingress:
  enabled: false

grafanaDashboard:
  enabled: true
  grafanaOperator:
    enabled: true
    matchLabels:
      grafana.com/dashboards: portefaix
    existingDashboards:
    - name: alertmanager-overview
      folder: monitoring
    - name: apiserver
      folder: kubernetes
    - name: cluster-total
      folder: kubernetes
    - name: controller-manager
      folder: kubernetes
    - name: etcd
      folder: kubernetes
    - name: grafana-overview
      folder: monitoring
    - name: k8s-coredns
      folder: kubernetes
    - name: k8s-resources-cluster
      folder: kubernetes
    - name: k8s-resources-multicluster
      folder: kubernetes
    - name: k8s-resources-namespace
      folder: kubernetes
    - name: k8s-resources-node
      folder: kubernetes
    - name: k8s-resources-pod
      folder: kubernetes
    - name: k8s-resources-workload
      folder: kubernetes
    - name: k8s-resources-workloads-namespace
      folder: kubernetes
    - name: kubelet
      folder: kubernetes
    - name: namespace-by-pod
      folder: kubernetes
    - name: namespace-by-workload
      folder: kubernetes
    - name: node-cluster-rsrc-use
      folder: compute
    - name: node-rsrc-use
      folder: compute
    - name: nodes
      folder: compute
    - name: nodes-darwin
      folder: compute
    - name: persistentvolumesusage
      folder: kubernetes
    - name: pod-total
      folder: kubernetes
    - name: prometheus
      folder: monitoring
    - name: proxy
      folder: kubernetes
    - name: scheduler
      folder: kubernetes
    - name: workload-total
      folder: kubernetes

cilium:
  enabled: false

# grafana:
#   # Grafana folder in which to store the dashboards
#   folder: compute

monitoring:
  runbookUrl: "https://runbooks.prometheus-operator.dev/runbooks"

kube-prometheus-stack:
  commonLabels:
    portefaix.xyz/version: v0.54.0

  crds:
    enabled: false

  defaultRules:
    create: true
    labels:
      prometheus.io/operator: portefaix
    # additionalRuleLabels:
    #   prometheus.io/operator: portefaix
    # TODO: use https://runbooks.portefaix.xyz
    runbookUrl: "https://runbooks.prometheus-operator.dev/runbooks"
    disabled:
      InfoInhibitor: true

  alertmanager:
    enabled: true
    annotations:
      a8r.io/description: Default Prometheus rules
      a8r.io/owner: prometheus-community
      a8r.io/bugs: https://github.com/prometheus-community/helm-charts/issues
      a8r.io/documentation: https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack
      a8r.io/repository: https://github.com/prometheus-community/helm-charts
      a8r.io/support: https://github.com/prometheus-community/helm-charts/issues
    serviceAccount:
      create: true
      name: alertmanager
    tplConfig: false
    ingress:
      enabled: false
    serviceMonitor:
      selfMonitor: true
      additionalLabels:
        prometheus.io/operator: portefaix

    alertmanagerSpec:
      # useExistingSecret: true
      # configSecret: alertmanager-config
      alertmanagerConfiguration:
        name: core
      logFormat: json
      logLevel: debug
      # secrets:
      # - alertmanager-credentials

  grafana:
    enabled: false
    forceDeployDashboards: true
    Other options are: utc, browser or a specific timezone, i.e. Europe/Paris
    defaultDashboardsTimezone: Europe/Paris
    sidecar:
      dashboards:
        enabled: true
        label: grafana.com/dashboard
        labelValue: kube-prometheus-stack
        annotations:
          grafana.com/folder: kubernetes

  kubeApiServer:
    enabled: true
    serviceMonitor:
      additionalLabels:
        prometheus.io/operator: portefaix

  kubelet:
    enabled: true
    serviceMonitor:
      additionalLabels:
        prometheus.io/operator: portefaix
      cAdvisorMetricRelabelings:
      # From KPS chart
      # Drop less useful container CPU metrics.
      - sourceLabels: [__name__]
        action: drop
        regex: "container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)"
      # Drop less useful container / always zero filesystem metrics.
      - sourceLabels: [__name__]
        action: drop
        regex: "container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)"
      # Drop less useful / always zero container memory metrics.
      - sourceLabels: [__name__]
        action: drop
        regex: "container_memory_(mapped_file|swap)"
      # Drop less useful container process metrics.
      - sourceLabels: [__name__]
        action: drop
        regex: "container_(file_descriptors|tasks_state|threads_max)"
      # Drop container spec metrics that overlap with kube-state-metrics.
      - sourceLabels: [__name__]
        action: drop
        regex: "container_spec.*"
      # Drop cgroup metrics with no pod.
      - sourceLabels: [id, pod]
        action: drop
        regex: ".+;"
      # End from KPS chart

      # See https://github.com/grafana/k8s-monitoring-helm/pull/326
      - sourceLabels: ["__name__", "image"]
        separator: "@"
        regex: "(container_cpu_.*|container_fs_.*|container_memory_.*|container_network_.*)@"
        action: drop
      - sourceLabels: ["__name__", "container"]
        separator: "@"
        regex: "(container_cpu_.*|container_fs_.*|container_memory_.*)@"
        action: drop

      # drop all of these labels, but no more dropping than just giving them all a common value, so that expressions such as image!="" still work
      - sourceLabels: [__name__]
        regex: "container_cpu_.*|container_fs_.*|container_memory_.*|container_network_.*"
        targetLabel: "id"
        replacement: "NA"
      - sourceLabels: [__name__]
        regex: "container_cpu_.*|container_fs_.*|container_memory_.*|container_network_.*"
        targetLabel: "image"
        replacement: "NA"
      - sourceLabels: [__name__]
        regex: "machine_memory_bytes"
        targetLabel: "boot_id"
        replacement: "NA"
      - sourceLabels: [__name__]
        regex: "machine_memory_bytes"
        targetLabel: "system_uuid"
        replacement: "NA"

      # filter all storage devices that isnt physical, this is a 2 stage rule, first whitelist the ones we cant to keep then drop the specific ones that was not whitelisted
      # notice we also drop the dm-, since cadvisor double-reports both the dm and the mapped device, so if sda is e.g. dm-0, then you get value for both
      - sourceLabels: ["__name__", "device"]
        separator: "@"
        regex: "container_fs_.*@(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dasd.+)"
        targetLabel: "__keepme"
        replacement: "1"
      - sourceLabels: ["__name__", "__keepme"]
        separator: "@"
        regex: "container_fs_.*@"
        action: drop
      - sourceLabels: ["__name__"]
        regex: "container_fs_.*"
        targetLabel: "__keepme"
        replacement: ""

      # filter all network devices that isnt physical, this is a 2 stage rule, first whitelist the ones we cant to keep then drop the specific ones that was not whitelisted
      - sourceLabels: ["__name__", "interface"]
        separator: "@"
        regex: "container_network_.*@(en[ospx]|wlan|eth)[0-9].*"
        targetLabel: "__keepme"
        replacement: "1"
      - sourceLabels: ["__name__", "__keepme"]
        separator: "@"
        regex: "container_network_.*@"
        action: drop
      - sourceLabels: ["__name__"]
        regex: "container_network_.*"
        targetLabel: "__keepme"
        replacement: ""

  kubeControllerManager:
    enabled: true
    serviceMonitor:
      enabled: true
      additionalLabels:
        prometheus.io/operator: portefaix

  coreDns:
    enabled: true
    serviceMonitor:
      additionalLabels:
        prometheus.io/operator: portefaix

  kubeDns:
    enabled: false
    serviceMonitor:
      additionalLabels:
        prometheus.io/operator: portefaix

  kubeEtcd:
    enabled: true
    serviceMonitor:
      enabled: true
      additionalLabels:
        prometheus.io/operator: portefaix

  kubeScheduler:
    enabled: true
    serviceMonitor:
      enabled: true
      additionalLabels:
        prometheus.io/operator: portefaix

  kubeProxy:
    enabled: true
    serviceMonitor:
      enabled: true
      additionalLabels:
        prometheus.io/operator: portefaix

  kubeStateMetrics:
    enabled: true

  kube-state-metrics:
    metricLabelsAllowlist:
    - nodes=[agentpool,node_kubernetes_io_instance-type]
    - pods=[*]
    - deployments=[*]
    - persistentvolumeclaims=[*]
    prometheus:
      monitor:
        enabled: true
        additionalLabels:
          prometheus.io/operator: portefaix
    rbac:
      extraRules:
      - apiGroups: ["autoscaling.k8s.io"]
        resources: ["verticalpodautoscalers"]
        verbs: ["list", "watch"]
    selfMonitor:
      enabled: true
    customResourceState:
      enabled: true
      config:
        kind: CustomResourceStateMetrics
        spec:
          resources:
          - groupVersionKind:
              group: autoscaling.k8s.io
              kind: "VerticalPodAutoscaler"
              version: "v1"
            labelsFromPath:
              verticalpodautoscaler: [metadata, name]
              namespace: [metadata, namespace]
              target_api_version: [spec, targetRef, apiVersion]
              target_kind: [spec, targetRef, kind]
              target_name: [spec, targetRef, name]
            metrics:
            - name: "verticalpodautoscaler_labels"
              help: "VPA container recommendations. Kubernetes labels converted to Prometheus labels"
              each:
                type: Info
                info:
                  labelsFromPath:
                    name: [metadata, name]
            - name: "verticalpodautoscaler_status_recommendation_containerrecommendations_target"
              help: "VPA container recommendations for memory. Target resources the VerticalPodAutoscaler recommends for the container."
              each:
                type: Gauge
                gauge:
                  path: [status, recommendation, containerRecommendations]
                  valueFrom: [target, memory]
                  labelsFromPath:
                    container: [containerName]
              commonLabels:
                resource: "memory"
                unit: "byte"
            - name: "verticalpodautoscaler_status_recommendation_containerrecommendations_lowerbound"
              help: "VPA container recommendations for memory. Minimum resources the container can use before the VerticalPodAutoscaler updater evicts it"
              each:
                type: Gauge
                gauge:
                  path: [status, recommendation, containerRecommendations]
                  valueFrom: [lowerBound, memory]
                  labelsFromPath:
                    container: [containerName]
              commonLabels:
                resource: "memory"
                unit: "byte"
            - name: "verticalpodautoscaler_status_recommendation_containerrecommendations_upperbound"
              help: "VPA container recommendations for memory. Maximum resources the container can use before the VerticalPodAutoscaler updater evicts it"
              each:
                type: Gauge
                gauge:
                  path: [status, recommendation, containerRecommendations]
                  valueFrom: [upperBound, memory]
                  labelsFromPath:
                    container: [containerName]
              commonLabels:
                resource: "memory"
                unit: "byte"
            - name: "verticalpodautoscaler_status_recommendation_containerrecommendations_uncappedtarget"
              help: "VPA container recommendations for memory. Target resources the VerticalPodAutoscaler recommends for the container ignoring bounds"
              each:
                type: Gauge
                gauge:
                  path: [status, recommendation, containerRecommendations]
                  valueFrom: [uncappedTarget, memory]
                  labelsFromPath:
                    container: [containerName]
              commonLabels:
                resource: "memory"
                unit: "byte"
            - name: "verticalpodautoscaler_status_recommendation_containerrecommendations_target"
              help: "VPA container recommendations for cpu. Target resources the VerticalPodAutoscaler recommends for the container."
              each:
                type: Gauge
                gauge:
                  path: [status, recommendation, containerRecommendations]
                  valueFrom: [target, cpu]
                  labelsFromPath:
                    container: [containerName]
              commonLabels:
                resource: "cpu"
                unit: "core"
            - name: "verticalpodautoscaler_status_recommendation_containerrecommendations_lowerbound"
              help: "VPA container recommendations for cpu. Minimum resources the container can use before the VerticalPodAutoscaler updater evicts it"
              each:
                type: Gauge
                gauge:
                  path: [status, recommendation, containerRecommendations]
                  valueFrom: [lowerBound, cpu]
                  labelsFromPath:
                    container: [containerName]
              commonLabels:
                resource: "cpu"
                unit: "core"
            - name: "verticalpodautoscaler_status_recommendation_containerrecommendations_upperbound"
              help: "VPA container recommendations for cpu. Maximum resources the container can use before the VerticalPodAutoscaler updater evicts it"
              each:
                type: Gauge
                gauge:
                  path: [status, recommendation, containerRecommendations]
                  valueFrom: [upperBound, cpu]
                  labelsFromPath:
                    container: [containerName]
              commonLabels:
                resource: "cpu"
                unit: "core"
            - name: "verticalpodautoscaler_status_recommendation_containerrecommendations_uncappedtarget"
              help: "VPA container recommendations for cpu. Target resources the VerticalPodAutoscaler recommends for the container ignoring bounds"
              each:
                type: Gauge
                gauge:
                  path: [status, recommendation, containerRecommendations]
                  valueFrom: [uncappedTarget, cpu]
                  labelsFromPath:
                    container: [containerName]
              commonLabels:
                resource: "cpu"
                unit: "core"

  nodeExporter:
    enabled: true

  prometheus-node-exporter:
    extraArgs:
    # Disable unused collectors
    - --no-collector.arp
    - --no-collector.ipvs
    - --no-collector.sockstat
    - --no-collector.softnet
    # Excludes from kube-prometheus-stack
    - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
    - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
    prometheus:
      monitor:
        enabled: true
        additionalLabels:
          prometheus.io/operator: portefaix
        # See https://github.com/grafana/k8s-monitoring-helm/pull/326
        metricRelabelings:
        - sourceLabels: ["__name__", "fstype"]
          separator: "@"
          regex: "node_filesystem.*@tmpfs"
          action: drop
        - sourceLabels: ["__name__", "mountpoint"]
          separator: "@"
          regex: "node_filesystem.*@/"
          targetLabel: "__keepme"
          action: drop
        - sourceLabels: ["__name__", "__keepme"]
          separator: "@"
          regex: "node_filesystem.*@"
          action: drop
        - sourceLabels: ["__name__"]
          regex: "node_filesystem.*@"
          targetLabel: "__keepme"
          replacement: ""

  prometheusOperator:
    enabled: true
    tls:
      enabled: false
    admissionWebhooks:
      enabled: false

    serviceAccount:
      create: true
      name: prometheus-operator
    serviceMonitor:
      selfMonitor: rue
      additionalLabels:
        prometheus.io/operator: portefaix
    logFormat: json
    logLevel: info

  prometheus:
    enabled: true
    annotations:
      a8r.io/description: Default Prometheus rules
      a8r.io/owner: prometheus-community
      a8r.io/bugs: https://github.com/prometheus-community/helm-charts/issues
      a8r.io/documentation: https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack
      a8r.io/repository: https://github.com/prometheus-community/helm-charts
      a8r.io/support: https://github.com/prometheus-community/helm-charts/issues
    serviceAccount:
      create: true
      name: prometheus
    ingress:
      enabled: false
    serviceMonitor:
      selfMonitor: true
      additionalLabels:
        prometheus.io/operator: portefaix
    prometheusSpec:
      enableAdminAPI: true
      enableRemoteWriteReceiver: true
      enableFeatures:
      - remote-write-receiver
      scrapeInterval: 30s
      evaluationInterval: 30s
      ruleNamespaceSelector: {}
      ruleSelector:
        matchLabels:
          prometheus.io/operator: portefaix
          # observability: portefaix
      serviceMonitorSelector:
        matchLabels:
          prometheus.io/operator: portefaix
          # observability: portefaix
      serviceMonitorNamespaceSelector: {}
      podMonitorSelector:
        matchLabels:
          prometheus.io/operator: portefaix
          # observability: portefaix
      podMonitorNamespaceSelector: {}
      probeSelector:
        matchLabels:
          prometheus.io/operator: portefaix
          # observability: portefaix
      probeNamespaceSelector: {}
      retention: 3d
      logLevel: info
      logFormat: json
      additionalScrapeConfigs: []

# kubernetes-mixin:
#   additionalLabels:
#     portefaix.xyz/version: v0.54.0
#     prometheus.io/operator: portefaix
#   additionalAnnotations: {}
#   monitor:
#     additionalLabels:
#       prometheus.io/operator: portefaix
#   grafana:
#     folder: kubernetes

# prometheus-mixin:
#   additionalLabels:
#     portefaix.xyz/version: v0.54.0
#     prometheus.io/operator: portefaix
#   additionalAnnotations: {}
#   monitor:
#     additionalLabels:
#       prometheus.io/operator: portefaix
#   grafana:
#     folder: monitoring

# prometheus-operator-mixin:
#   additionalLabels:
#     portefaix.xyz/version: v0.54.0

#   monitor:
#     additionalLabels:
#       prometheus.io/operator: portefaix
#   grafana:
#     folder: monitoring

# node-exporter-mixin:
#   additionalLabels:
#     portefaix.xyz/version: v0.54.0

#   additionalAnnotations: {}
#   monitor:
#     additionalLabels:
#       prometheus.io/operator: portefaix
#   grafana:
#     folder: monitoring

# kube-state-metrics-mixin:
#   additionalLabels:
#     portefaix.xyz/version: v0.54.0

#   additionalAnnotations: {}
#   monitor:
#     additionalLabels:
#       prometheus.io/operator: portefaix
#   grafana:
#     folder: kubernetes

# alertmanager-mixin:
#   additionalLabels:
#     portefaix.xyz/version: v0.54.0

#   additionalAnnotations: {}
#   monitor:
#     additionalLabels:
#       prometheus.io/operator: portefaix
#   grafana:
#     folder: kubernetes

coredns-mixin:
  additionalLabels:
    portefaix.xyz/version: v0.54.0

  additionalAnnotations: {}

  monitor:
    additionalLabels:
      prometheus.io/operator: portefaix

  grafanaDashboard:
    enabled: true
    folder: kubernetes
    grafanaOperator:
      enabled: true
      matchLabels:
        grafana.com/dashboards: portefaix

blackboxExporter:
  endpoint: blackbox-exporter-prometheus-blackbox-exporter.monitoring:9115
  probes:
    portefaix: []
    tailscale: []

pyrra-service-levels:
  additionalLabels:
    portefaix.xyz/version: v0.54.0
  pyrra:
    enabled: true
    labels:
      pyrra.dev/role: monitoring
    slos:
      # -- Latency SLOs
      latency:
      - name: prometheus-operator-reconcile-latency
        service: prometheus-operator
        dashboard: http://grafana.192.168.0.61.nip.io
        runbook: https://notions.so
        message: Prometheus Operator reconcilation have latency
        team: "sre"
        extraLabels: {}
        metric: prometheus_operator_reconcile_duration_seconds_bucket{job="kube-prometheus-stack-operator", namespace="monitoring", le="0.1"}
        metricTotal: prometheus_operator_reconcile_duration_seconds_count{job="kube-prometheus-stack-operator", namespace="monitoring"}
        groupBy:
        - controller
        window: 1d
        alerting:
          name: SLOPrometheusOperatorReconciliationLatenctErrorBudgetBurning
          absent: true
          burnrates: true
          disabled: false

      # -- Ration SLOs
      ratio:
      - name: prometheus-operator-http-errors
        service: prometheus-operator
        dashboard: http://grafana.192.168.0.61.nip.io
        runbook: https://notions.so
        message: Prometheus Operator API have errors
        team: "sre"
        extraLabels: {}
        metric: prometheus_operator_kubernetes_client_http_requests_total{job="kube-prometheus-stack-operator", namespace="monitoring", status_code=~"5.."}
        metricTotal: prometheus_operator_kubernetes_client_http_requests_total{job="kube-prometheus-stack-operator", namespace="monitoring"}
        groupBy: []
        target: "99.5"
        window: 1d
        alerting:
          name: SLOPrometheusOperatorAvailabilityErrorBudgetBurning
          absent: true
          burnrates: true
          disabled: false
      - name: prometheus-operator-reconcile-errors
        service: prometheus-operator
        dashboard: http://grafana.192.168.0.61.nip.io
        runbook: https://notions.so
        message: Prometheus Operator reconciliation have errors
        team: "sre"
        extraLabels: {}
        metric: prometheus_operator_reconcile_errors_total{job="kube-prometheus-stack-operator",namespace="monitoring"}
        metricTotal: prometheus_operator_reconcile_operations_total{job="kube-prometheus-stack-operator", namespace="monitoring"}
        groupBy: []
        target: "95"
        window: 1d
        alerting:
          name: SLOPrometheusOperatorAvailabilityErrorBudgetBurning
          absent: true
          burnrates: true
          disabled: false
      - name: prometheus-notifications-errors
        service: prometheus
        dashboard: http://grafana.192.168.0.61.nip.io
        runbook: https://notions.so
        message: ""
        team: "sre"
        extraLabels: {}
        metric: prometheus_notifications_errors_total{job="kube-prometheus-stack-prometheus", namespace="monitoring"}
        metricTotal: prometheus_notifications_sent_total{job="kube-prometheus-stack-prometheus", namespace="monitoring"}
        groupBy: []
        target: "99"
        window: 1d
        alerting:
          name: SLOPrometheusNotificationsAvailabilityErrorBudgetBurning
          absent: true
          burnrates: true
          disabled: false
      - name: prometheus-query-errors
        service: prometheus
        dashboard: http://grafana.192.168.0.61.nip.io
        runbook: https://notions.so
        message: "95% of Prometheus requests return a good HTTP code"
        team: "sre"
        extraLabels: {}
        metric: prometheus_http_requests_total{job="kube-prometheus-stack-prometheus", namespace="monitoring", handler=~"/api.*", code=~"5.."}
        metricTotal: prometheus_http_requests_total{job="kube-prometheus-stack-prometheus", namespace="monitoring", handler=~"/api.*"}
        groupBy:
        - handler
        target: "99"
        window: 1d
        alerting:
          name: SLOPrometheusAPIAvailabilityErrorBudgetBurning
          absent: true
          burnrates: true
          disabled: false
      - name: prometheus-rule-evaluation-failures
        service: prometheus
        dashboard: http://grafana.192.168.0.61.nip.io
        runbook: https://notions.so
        message: ""
        team: "sre"
        extraLabels: {}
        metric: prometheus_rule_evaluation_failures_total{job="kube-prometheus-stack-prometheus", namespace="monitoring"}
        metricTotal: prometheus_rule_evaluations_total{job="kube-prometheus-stack-prometheus", namespace="monitoring"}
        groupBy: []
        target: "99"
        window: 1d
        alerting:
          name: SLOPrometheusRuleEvaluationAvailabilityErrorBudgetBurning
          absent: true
          burnrates: true
          disabled: false
      - name: prometheus-sd-kubernetes-errors
        service: prometheus
        dashboard: http://grafana.192.168.0.61.nip.io
        runbook: https://notions.so
        message: "Prometheus have error with Kubernetes Service Discovery"
        team: "sre"
        extraLabels: {}
        metric: prometheus_sd_kubernetes_http_request_total{job="kube-prometheus-stack-prometheus", namespace="monitoring", status_code=~"5..|<error>"}
        metricTotal: prometheus_sd_kubernetes_http_request_total{job="kube-prometheus-stack-prometheus", namespace="monitoring"}
        groupBy: []
        target: "99"
        window: 1d
        alerting:
          name: SLOPrometheusServiceDiscoveryAvailabilityErrorBudgetBurning
          absent: true
          burnrates: true
          disabled: false
      - name: alertmanager-notification-errors
        service: alertmanager
        dashboard: http://grafana.192.168.0.61.nip.io
        runbook: https://notions.so
        message:
        team: "sre"
        extraLabels: {}
        metric: alertmanager_notifications_failed_total{job="kube-prometheus-stack-alertmanager", namespace="monitoring", code=~"^5..$"}
        metricTotal: alertmanager_notifications_failed_total{job="kube-prometheus-stack-alertmanager", namespace="monitoring", code!~"^4..$"}
        groupBy: []
        target: "99"
        window: 1d
        alerting:
          name: SLOAlertmanagerNotificationsAvailabilityErrorBudgetBurning
          absent: true
          burnrates: true
          disabled: false

      # -- Bool SLOs
      bool: []
      # - name: bool
      #   description: ""
      #   team: "sre"
      #   extraLabels: {}
      #   metric: http_requests{status="500"} > 0
      #   target: 0.01
      #   alerting:
      #     name: ErrorBudgetBurn
      #     absent: true
      #     burnrates: true
      #     disabled: false
