{{ define "alloy.config.traces" }}

// ********************************************
// * O P E N T E L E M E T R Y / T R A C E S
// ********************************************

otelcol.receiver.otlp "core" {
  grpc {
    endpoint = "0.0.0.0:4317"
  }
  http {
    endpoint = "0.0.0.0:4318"
  }

  output {
    traces = [
      otelcol.processor.batch.core.input,
      otelcol.connector.spanlogs.autologging.input,
      // Uncomment the next line to generate service graph metrics from the Alloy.
      // By default this is generated by the Tempo component, so be sure to remove the relevant configuration
      // otelcol.connector.servicegraph.tracemetrics.input,
      // Uncomment the next line to generate span metrics from the Alloy.
      // By default this is generated by the Tempo component, so be sure to remove the relevant configuration
      // otelcol.connector.spanmetrics.tracemetrics.input,
      // The following would be used for tail sampling only traces containing errors.
      //otelcol.processor.tail_sampling.errors.input,
    ]
  }
}

otelcol.processor.batch "core" {
  // Wait until we've received 16K of data.
  send_batch_size = 16384
  // Or until 5 seconds have elapsed.
  timeout = "5s"

  output {
    traces = [otelcol.processor.attributes.core.input]
  }
}

otelcol.processor.attributes "core" {
  action {
    key = "cluster"
    // value = {{ .Values.observability.cluster_name | quote }}
    value = local.file.cluster_name.content
    action = "insert"
  }

  output {
    traces = [otelcol.processor.k8sattributes.core.input]
  }
}

otelcol.processor.k8sattributes "core" {
  extract {
    metadata = [
      "k8s.namespace.name",
      "k8s.pod.name",
      "k8s.deployment.name",
      "k8s.statefulset.name",
      "k8s.daemonset.name",
      "k8s.cronjob.name",
      "k8s.job.name",
      "k8s.node.name",
      "k8s.pod.uid",
      "k8s.pod.start_time",
    ]
  }

  pod_association {
    source {
      from = "connection"
    }
  }

  output {
    traces = [otelcol.processor.memory_limiter.core.input]
  }
}

otelcol.processor.memory_limiter "core" {
  limit_percentage = 90
  spike_limit_percentage = 30
  check_interval = "5s"

  output {
    traces  = [
      otelcol.exporter.otlp.local.input,
      // otelcol.exporter.otlp.grafana_cloud.input,
    ]
  }
}

// -------------------------------------------------------------------------------------------------------
// Spanlog connector processes incoming trace spans and extracts data from them ready for logging

otelcol.connector.spanlogs "autologging" {
  // We only want to output a line for each root span (ie. every single trace), and not for every
  // process or span (outputting a line for every span would be extremely verbose).
  spans = false
  roots = true
  processes = false
  // We want to ensure that the following three span attributes are included in the log line, if
  // present.
  span_attributes = [ "http.method", "http.target", "http.status_code" ]

  // Overrides the default key in the log line to be `traceId`, which is then used by Grafana to
  // identify the trace ID for correlation with the Tempo datasource.
  overrides {
    trace_id_key = "traceId"
  }

  output {
    logs = [
      otelcol.exporter.loki.autologging.input,
    ]
  }
}

// Simply forwards the incoming OpenTelemetry log format out as a Loki log.
// We need this stage to ensure we can then process the logline as a Loki object.
otelcol.exporter.loki "autologging" {
  forward_to = [
    loki.process.autologging.receiver,
  ]
}

// The Loki processor allows us to accept a correctly formatted Loki log and mutate it into
// a set of fields for output.
loki.process "autologging" {
  // The JSON stage simply extracts the `body` (the actual logline) from the Loki log, ignoring
  // all other fields.
  stage.json {
    expressions = { "body" = "" }
  }

  // The output stage takes the body (the main logline) and uses this as the source for the output
  // logline. In this case, it essentially turns it into logfmt.
  stage.output {
    source = "body"
  }

  forward_to = [
    loki.write.local.receiver,
    // loki.write.grafana_cloud.receiver,
  ]
}

// -------------------------------------------------------------------------------------------------------
// Tail Sampling processor will use a set of policies to determine which received traces to keep
// and send to Tempo.

// otelcol.processor.tail_sampling "errors" {
//   // Total wait time from the start of a trace before making a sampling decision. Note that smaller time
//   // periods can potentially cause a decision to be made before the end of a trace has occurred.
//   decision_wait = "30s"

//   // This policy defines that traces that contain errors should be kept.
//   policy {
//     // The name of the policy can be used for logging purposes.
//     name = "sample-erroring-traces"
//     // The type must match the type of policy to be used, in this case examing the status code
//     // of every span in the trace.
//     type = "status_code"
//     // This block determines the error codes that should match in order to keep the trace,
//     // in this case the OpenTelemetry 'ERROR' code.
//     status_code {
//       status_codes = [ "ERROR" ]
//     }
//   }

//   // This policy defines that only traces that are longer than 200ms in total should be kept.
//   policy {
//     // The name of the policy can be used for logging purposes.
//     name = "sample-long-traces"
//     // The type must match the policy to be used, in this case the total latency of the trace.
//     type = "latency"
//     // This block determines the total length of the trace in milliseconds.
//     latency {
//       threshold_ms = 200
//     }
//   }

//   output {
//     traces = [
//       otelcol.processor.batch.core.input,
//       ]
//   }
// }

// -------------------------------------------------------------------------------------------------------
// The Spanmetrics Connector will generate RED metrics based on the incoming trace span data.

// otelcol.connector.spanmetrics "tracemetrics" {
//   // The namespace explicit adds a prefix to all the generated span metrics names.
//   // In this case, we'll ensure they match as closely as possible those generated by Tempo.
//   namespace = "traces.spanmetrics"

//   // Each extra dimension (metrics label) to be added to the generated metrics from matching span attributes. These
//   // need to be defined with a name and optionally a default value (in the following cases, we do not want a default
//   // value if the span attribute is not present).
//   dimension {
//     name = "http.method"
//   }
//   dimension {
//     name = "http.target"
//   }
//   dimension {
//     name = "http.status_code"
//   }
//   dimension {
//     name = "service.version"
//   }

//   // A histogram block must be present, either explicitly defining bucket values or via an exponential block.
//   // We do the latter here.
//   histogram {
//     explicit {
//     }
//   }

//   // The exemplar block is added to ensure we generate exemplars for traces on relevant metric values.
//   exemplars {
//     enabled = true
//   }

//   output {
//     metrics = [
//       otelcol.exporter.prometheus.tracemetrics.input,
//     ]
//   }
// }

// -------------------------------------------------------------------------------------------------------
// The Servicegraph Connector will generate service graph metrics (edges and nodes) based on incoming trace spans.

// otelcol.connector.servicegraph "tracemetrics" {
//   // Extra dimensions (metrics labels) to be added to the generated metrics from matching span attributes.
//   // For this component, this is defined as an array. There are no default values and the labels will not be generated
//   // for missing span attributes.
//   dimensions = [
//     "http.method",
//     "http.target",
//     "http.status_code",
//     "service.version",
//   ]

//   // Generated metrics data is in OTLP format. We send this data to the OpenTelemetry Prometheus exporter to ensure
//   // it gets transformed into Prometheus format data.
//   output {
//     metrics = [
//       otelcol.exporter.prometheus.tracemetrics.input,
//     ]
//   }
// }

// The OpenTelemetry Prometheus exporter will transform incoming OTLP metrics data into Prometheus format data.
otelcol.exporter.prometheus "tracemetrics" {
  forward_to = [
    prometheus.remote_write.local.receiver,
    // prometheus.remote_write.grafana_cloud.receiver,
  ]
}

{{ end }}
