---
# SPDX-FileCopyrightText: Copyright (C) Nicolas Lamirault <nicolas.lamirault@gmail.com>
# SPDX-License-Identifier: Apache-2.0

grafanaDashboard:
  enabled: true
  grafanaOperator:
    enabled: true
    matchLabels:
      grafana.com/dashboards: portefaix

opentelemetry-logs:
  additionalLabels:
    app.kubernetes.io/part-of: opentelemetry-collector
    portefaix.xyz/version: v0.54.0

  mode: daemonset
  image:
    repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib

  useGOMEMLIMIT: true

  presets:
    logsCollection:
      enabled: true
      includeCollectorLogs: true
    hostMetrics:
      enabled: false
    kubernetesAttributes:
      enabled: true
    kubeletMetrics:
      enabled: false
    kubernetesEvents:
      enabled: false
    clusterMetrics:
      enabled: false

  config:
    receivers:
      jaeger: {}
      otlp: {}
      zipkin: {}
      filelog:
        exclude: []
        include:
        - /var/log/pods/*/*/*.log
        include_file_name: false
        include_file_path: true
        operators:
        - id: container-parser
          max_log_size: 102400
          type: container
        # Extract metadata from file path
        - type: regex_parser
          id: extract_metadata_from_filepath
          regex: '^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]+)\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$'
          parse_from: attributes["log.file.path"]
          output: add_cluster_name
          # Add cluster name attribute from environment variable
        - id: add_cluster_name
          type: add
          field: resource["k8s.cluster.name"]
          value: EXPR(env("K8S_CLUSTER_NAME"))
          output: move_stream
        retry_on_failure:
          enabled: true
        start_at: end


    processors:
      resourcedetection:
        detectors:
        - k8snode
        - env
        - system
        timeout: 2s
        override: false
      k8sattributes:
        extract:
          labels:
          - from: pod
            key: app.kubernetes.io/name
            tag_name: service.name
          - from: pod
            key: app.kubernetes.io/name
            tag_name: k8s.app.name
          - from: pod
            key: k8s-app
            tag_name: service.name
          - from: pod
            key: app.kubernetes.io/instance
            tag_name: k8s.app.instance
          - from: pod
            key: app.kubernetes.io/version
            tag_name: service.version
          - from: pod
            key: app.kubernetes.io/component
            tag_name: k8s.app.component
          - from: pod
            key: app.kubernetes.io/part-of
            tag_name: k8s.app.part_of
          - from: pod
            key: app.kubernetes.io/managed-by
            tag_name: k8s.app.managed_by
          # Extract all labels
          # - tag_name: $$1
          #   key_regex: (.*)
          #   from: pod
          # annotations:
          # Extract all annotations
          # - tag_name: $$1
          #   key_regex: (.*)
          #   from: pod
          metadata:
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.node.name
          - k8s.namespace.name
          - k8s.pod.start_time
          - k8s.replicaset.name
          - k8s.replicaset.uid
          - k8s.daemonset.name
          - k8s.daemonset.uid
          - k8s.job.name
          - k8s.job.uid
          - k8s.cronjob.name
          - k8s.statefulset.name
          - k8s.statefulset.uid
          - container.image.name
          - container.image.tag
          - k8s.container.name
          - container.id
          - container.image.name
          - container.image.tag
          - container.id
          - k8s.cluster.uid
        filter:
          node_from_env_var: K8S_NODE_NAME
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
          - from: resource_attribute
            name: k8s.node.name
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
        - sources:
          - from: connection
      transform/core:
        error_mode: ignore
        log_statements:
        - context: resource
          statements:
          - set(attributes["loki.attribute.labels"], "node, deployment, namespace, container, pod, app")
        - conditions:
          - IsMatch(body, "^\\{")
          context: log
          statements:
          - merge_maps(cache, ParseJSON(body), "upsert")
          - set(severity_text, "") where cache["level"] == nil
          - set(severity_text, ConvertCase(cache["level"],"upper")) where cache["level"] != nil
          - set(trace_id.string, String(cache["TRACE_ID"]))
          - set(span_id.string, String(cache["SPAN_ID"]))
          - set(time, cache["time"])
          - set(instrumentation_scope.name,cache["service.name"])
          - set(instrumentation_scope.version,cache["service.version"])
        - context: log
          statements:
          - set(severity_text, ConvertCase(attributes["level"], "upper")) where IsMatch(attributes["level"], "(?i)^(trace|debug|info|warn|error|fatal)$") == true
          - delete_key(attributes, "level") where attributes["level"] != nil and severity_text == ConvertCase(attributes["level"], "upper")
        - conditions:
          - severity_text == ""
          context: log
          statements:
          - set(severity_text, "INFO") where IsString(body) and IsMatch(body, "\\sINFO\\s")
          - set(severity_text, "WARN") where IsString(body) and IsMatch(body, "\\sWARN\\s")
          - set(severity_text, "ERROR") where IsString(body) and IsMatch(body, "\\sERROR\\s")
          - set(severity_text, "DEBUG") where IsString(body) and IsMatch(body, "\\sDEBUG\\s")
          - set(severity_text, "TRACE") where IsString(body) and IsMatch(body, "\\sTRACE\\s")
          - set(severity_text, "FATAL") where IsString(body) and IsMatch(body, "\\sFATAL\\s")
        - conditions:
          - severity_text == ""
          context: log
          statements:
          - set(severity_text, "UNKNOWN")
        - conditions:
          - severity_number == 0
          context: log
          statements:
          - set(severity_number, SEVERITY_NUMBER_TRACE) where severity_text=="TRACE"
          - set(severity_number, SEVERITY_NUMBER_DEBUG) where severity_text=="DEBUG"
          - set(severity_number, SEVERITY_NUMBER_INFO) where severity_text=="INFO"
          - set(severity_number, SEVERITY_NUMBER_WARN) where severity_text=="WARN"
          - set(severity_number, SEVERITY_NUMBER_ERROR) where severity_text=="ERROR"
          - set(severity_number, SEVERITY_NUMBER_FATAL) where severity_text=="FATAL"

      # redaction/card-numbers:
      #   allow_all_keys: true
      #   blocked_values:
      #     - "4[0-9]{12}(?:[0-9]{3})?" ## VISA
      #     - "(5[1-5][0-9]{14}|2(22[1-9][0-9]{12}|2[3-9][0-9]{13}|[3-6][0-9]{14}|7[0-1][0-9]{13}|720[0-9]{12}))" ## MasterCard
      #     - "3(?:0[0-5]|[68][0-9])[0-9]{11}" ## Diners Club
      #     - "3[47][0-9]{13}" ## American Express
      #     - "65[4-9][0-9]{13}|64[4-9][0-9]{13}|6011[0-9]{12}|(622(?:12[6-9]|1[3-9][0-9]|[2-8][0-9][0-9]|9[01][0-9]|92[0-5])[0-9]{10})" ## Discover
      #     - "(?:2131|1800|35[0-9]{3})[0-9]{11}" ## JCB
      #     - "62[0-9]{14,17}" ## UnionPay
      #   summary: debug

    exporters:
      otlphttp/gateway:
        endpoint: http://opentelemetry-collector-opentelemetry-gateway.opentelemetry.svc.cluster.local:4318

    service:
      telemetry:
        logs:
          level: info
          encoding: json
        metrics:
          address: ${env:MY_POD_IP}:8888
      pipelines:
        logs:
          receivers:
          - filelog
          processors:
          - resourcedetection
          - k8sattributes
          - memory_limiter
          - transform/core
          - batch
          exporters:
          # https://artifacthub.io/packages/helm/opentelemetry-helm/opentelemetry-collector#warning-warning-risk-of-looping-the-exported-logs-back-into-the-receiver-causing-log-explosion
          # - debug
          - otlphttp/gateway
        metrics: {}
        traces: {}

  ports:
    otlp:
      enabled: false
      containerPort: 4317
      servicePort: 4317
      protocol: TCP
      appProtocol: grpc
    otlp-http:
      enabled: false
      containerPort: 4318
      servicePort: 4318
      protocol: TCP
    jaeger-compact:
      enabled: false
    jaeger-thrift:
      enabled: false
    jaeger-grpc:
      enabled: false
    zipkin:
      enabled: false
    metrics:
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP

  securityContext:
    runAsUser: 0
    runAsGroup: 0

  service:
    enabled: true

  serviceMonitor:
    enabled: true
    extraLabels:
      prometheus.io/operator: portefaix

  prometheusRule:
    enabled: true
    groups: []
    defaultRules:
      enabled: false
    extraLabels:
      prometheus.io/operator: portefaix

opentelemetry-metrics:
  additionalLabels:
    app.kubernetes.io/part-of: opentelemetry-collector
    portefaix.xyz/version: v0.54.0

  mode: daemonset
  image:
    repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib

  useGOMEMLIMIT: true

  presets:
    logsCollection:
      enabled: false
    hostMetrics:
      enabled: true
    kubernetesAttributes:
      enabled: true
    kubeletMetrics:
      enabled: true
    kubernetesEvents:
      enabled: false
    clusterMetrics:
      enabled: false

  config:
    receivers:
      # https://github.com/helm/helm/pull/12879
      jaeger: null
      otlp: null
      zipkin: null
      kubeletstats:
        insecure_skip_verify: true
        metric_groups:
        - container
        - node
        - pod
        - volume
        metrics:
          container.cpu.usage:
            enabled: true
          container.uptime:
            enabled: true
          k8s.container.cpu_limit_utilization:
            enabled: true
          k8s.container.cpu_request_utilization:
            enabled: true
          k8s.container.memory_limit_utilization:
            enabled: true
          k8s.container.memory_request_utilization:
            enabled: true
          k8s.node.cpu.usage:
            enabled: true
          k8s.node.uptime:
            enabled: true
          k8s.pod.cpu.usage:
            enabled: true
          k8s.pod.cpu_limit_utilization:
            enabled: true
          k8s.pod.cpu_request_utilization:
            enabled: true
          k8s.pod.memory_limit_utilization:
            enabled: true
          k8s.pod.memory_request_utilization:
            enabled: true
          k8s.pod.uptime:
            enabled: true
        node: ${env:K8S_NODE_NAME}
      hostmetrics:
        scrapers:
          cpu:
            metrics:
              system.cpu.logical.count:
                enabled: true
          memory:
            metrics:
              system.memory.utilization:
                enabled: true
              system.memory.limit:
                enabled: true
          load: {}
          disk: {}
          filesystem:
            exclude_fs_types:
              fs_types:
              - autofs
              - binfmt_misc
              - bpf
              - cgroup2
              - configfs
              - debugfs
              - devpts
              - devtmpfs
              - fusectl
              - hugetlbfs
              - iso9660
              - mqueue
              - nsfs
              - overlay
              - proc
              - procfs
              - pstore
              - rpc_pipefs
              - securityfs
              - selinuxfs
              - squashfs
              - sysfs
              - tracefs
              match_type: strict
            exclude_mount_points:
              match_type: regexp
              mount_points:
              - /dev/*
              - /proc/*
              - /sys/*
              - /run/k3s/containerd/*
              - /var/lib/docker/*
              - /var/lib/kubelet/*
              - /snap/*
            metrics:
              system.filesystem.utilization:
                enabled: true
          network:
            exclude:
              interfaces:
                - ^veth.*$
                - ^docker.*$
                - ^br-.*$
                - ^flannel.*$
                - ^cali.*$
                - ^cbr.*$
                - ^cni.*$
                - ^dummy.*$
                - ^tailscale.*$
                - ^lo$
              match_type: regexp
          paging: {}
          processes: {}
          process: {}
            # mute_process_user_error: true
            # metrics:
            #   process.cpu.utilization:
            #     enabled: true
            #   process.memory.utilization:
            #     enabled: true
            #   process.threads:
            #     enabled: true
            #   process.paging.faults:
            #     enabled: true


    processors:
      resourcedetection:
        detectors:
        - k8snode
        - env
        - system
        timeout: 2s
        override: false
      k8sattributes:
        extract:
          labels:
          - from: pod
            key: app.kubernetes.io/name
            tag_name: service.name
          - from: pod
            key: app.kubernetes.io/name
            tag_name: k8s.app.name
          - from: pod
            key: k8s-app
            tag_name: service.name
          - from: pod
            key: app.kubernetes.io/instance
            tag_name: k8s.app.instance
          - from: pod
            key: app.kubernetes.io/version
            tag_name: service.version
          - from: pod
            key: app.kubernetes.io/component
            tag_name: k8s.app.component
          - from: pod
            key: app.kubernetes.io/part-of
            tag_name: k8s.app.part_of
          - from: pod
            key: app.kubernetes.io/managed-by
            tag_name: k8s.app.managed_by
          # Extract all labels
          # - tag_name: $$1
          #   key_regex: (.*)
          #   from: pod
          # annotations:
          # Extract all annotations
          # - tag_name: $$1
          #   key_regex: (.*)
          #   from: pod
          metadata:
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.node.name
          - k8s.namespace.name
          - k8s.pod.start_time
          - k8s.replicaset.name
          - k8s.replicaset.uid
          - k8s.daemonset.name
          - k8s.daemonset.uid
          - k8s.job.name
          - k8s.job.uid
          - k8s.cronjob.name
          - k8s.statefulset.name
          - k8s.statefulset.uid
          - container.image.name
          - container.image.tag
          - k8s.container.name
          - container.id
          - container.image.name
          - container.image.tag
          - container.id
          - k8s.cluster.uid
        filter:
          node_from_env_var: K8S_NODE_NAME
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
          - from: resource_attribute
            name: k8s.node.name
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
        - sources:
          - from: connection
      # transform:
      #   metric_statements:
      #   - context: resource
      #     statements:
      #     - set(attributes["k8s.cluster.name"], "${K8S_CLUSTER_NAME}")
      #     - set(attributes["cluster"], "${K8S_CLUSTER_NAME}")
      #     - set(attributes["deployment.environment.name"], "${K8S_ENVIRONMENT_NAME}")

    exporters:
      otlphttp/gateway:
        endpoint: http://opentelemetry-collector-opentelemetry-gateway.opentelemetry.svc.cluster.local:4318

    service:
      telemetry:
        logs:
          level: info
          encoding: json
        metrics:
          address: ${env:MY_POD_IP}:8888
      pipelines:
        logs: null
        metrics:
          receivers:
          # - otlp
          - prometheus
          processors:
          - resourcedetection
          - k8sattributes
          - memory_limiter
          - transform
          - batch
          exporters:
          - debug
          - otlphttp/gateway
        traces: null

  ports:
    otlp:
      enabled: false
      containerPort: 4317
      servicePort: 4317
      protocol: TCP
      appProtocol: grpc
    otlp-http:
      enabled: false
      containerPort: 4318
      servicePort: 4318
      protocol: TCP
    jaeger-compact:
      enabled: false
    jaeger-thrift:
      enabled: false
    jaeger-grpc:
      enabled: false
    zipkin:
      enabled: false
    metrics:
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP

  service:
    enabled: true

  serviceMonitor:
    enabled: true
    extraLabels:
      prometheus.io/operator: portefaix

  prometheusRule:
    enabled: true
    groups: []
    defaultRules:
      enabled: false
    extraLabels:
      prometheus.io/operator: portefaix

opentelemetry-metrics-cluster:
  additionalLabels:
    app.kubernetes.io/part-of: opentelemetry-collector
    portefaix.xyz/version: v0.54.0

  mode: deployment
  image:
    repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib

  useGOMEMLIMIT: true

  presets:
    logsCollection:
      enabled: false
    hostMetrics:
      enabled: false
    kubernetesAttributes:
      enabled: true
    kubeletMetrics:
      enabled: false
    kubernetesEvents:
      enabled: true
    clusterMetrics:
      enabled: true

  config:
    receivers:
      jaeger: null
      otlp: null
      zipkin: null
      k8s_cluster:
        collection_interval: 30s
        allocatable_types_to_report:
        - cpu
        - memory
        - storage
        node_conditions_to_report:
        - Ready
        - MemoryPressure
        - DiskPressure
        - NetworkUnavailable
        distribution: kubernetes
        metrics:
          k8s.node.condition:
            enabled: true
          k8s.pod.status_reason:
            enabled: true
      k8sobjects:
        objects:
        - exclude_watch_type:
          - DELETED
          group: events.k8s.io
          mode: watch
          name: events

    processors:
      resourcedetection:
        detectors:
        - k8snode
        - env
        - system
        timeout: 2s
        override: false
      k8sattributes:
        extract:
          labels:
          - from: pod
            key: app.kubernetes.io/name
            tag_name: service.name
          - from: pod
            key: app.kubernetes.io/name
            tag_name: k8s.app.name
          - from: pod
            key: k8s-app
            tag_name: service.name
          - from: pod
            key: app.kubernetes.io/instance
            tag_name: k8s.app.instance
          - from: pod
            key: app.kubernetes.io/version
            tag_name: service.version
          - from: pod
            key: app.kubernetes.io/component
            tag_name: k8s.app.component
          - from: pod
            key: app.kubernetes.io/part-of
            tag_name: k8s.app.part_of
          - from: pod
            key: app.kubernetes.io/managed-by
            tag_name: k8s.app.managed_by
          # Extract all labels
          # - tag_name: $$1
          #   key_regex: (.*)
          #   from: pod
          # annotations:
          # Extract all annotations
          # - tag_name: $$1
          #   key_regex: (.*)
          #   from: pod
          metadata:
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.node.name
          - k8s.namespace.name
          - k8s.pod.start_time
          - k8s.replicaset.name
          - k8s.replicaset.uid
          - k8s.daemonset.name
          - k8s.daemonset.uid
          - k8s.job.name
          - k8s.job.uid
          - k8s.cronjob.name
          - k8s.statefulset.name
          - k8s.statefulset.uid
          - container.image.name
          - container.image.tag
          - k8s.container.name
          - container.id
          - container.image.name
          - container.image.tag
          - container.id
          - k8s.cluster.uid
        filter:
          node_from_env_var: K8S_NODE_NAME
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
          - from: resource_attribute
            name: k8s.node.name
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
        - sources:
          - from: connection
      transform:
        metric_statements:
        - context: resource
          statements:
          - set(attributes["k8s.cluster.name"], "${K8S_CLUSTER_NAME}")
          - set(attributes["cluster"], "${K8S_CLUSTER_NAME}")
          - set(attributes["deployment.environment.name"], "${K8S_ENVIRONMENT_NAME}")

    exporters:
      otlphttp/gateway:
        endpoint: http://opentelemetry-collector-opentelemetry-gateway.opentelemetry.svc.cluster.local:4318

    service:
      telemetry:
        logs:
          level: info
          encoding: json
        metrics:
          address: ${env:MY_POD_IP}:8888
      pipelines:
        logs:
          receivers:
          - k8sobjects
          processors:
          - resourcedetection
          - k8sattributes
          - memory_limiter
          - batch
          exporters:
          - debug
          - otlphttp/gateway
        metrics:
          receivers:
          # - otlp
          - prometheus
          - k8s_cluster
          processors:
          - resourcedetection
          - k8sattributes
          - memory_limiter
          - transform
          - batch
          exporters:
          - debug
          - otlphttp/gateway
        traces: null

  ports:
    otlp:
      enabled: false
      containerPort: 4317
      servicePort: 4317
      protocol: TCP
      appProtocol: grpc
    otlp-http:
      enabled: false
      containerPort: 4318
      servicePort: 4318
      protocol: TCP
    jaeger-compact:
      enabled: false
    jaeger-thrift:
      enabled: false
    jaeger-grpc:
      enabled: false
    zipkin:
      enabled: false
    metrics:
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP

  service:
    enabled: true

  serviceMonitor:
    enabled: true
    extraLabels:
      prometheus.io/operator: portefaix

  prometheusRule:
    enabled: true
    groups: []
    defaultRules:
      enabled: true
    extraLabels:
      prometheus.io/operator: portefaix

opentelemetry-traces:
  additionalLabels:
    app.kubernetes.io/part-of: opentelemetry-collector
    portefaix.xyz/version: v0.54.0

  mode: deployment
  image:
    repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib

  useGOMEMLIMIT: true

  presets:
    logsCollection:
      enabled: false
    hostMetrics:
      enabled: false
    kubernetesAttributes:
      enabled: true
    kubeletMetrics:
      enabled: false
    kubernetesEvents:
      enabled: false
    clusterMetrics:
      enabled: false

  config:
    receivers:
      jaeger: null
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      zipkin: null

    processors:
      resourcedetection:
        detectors:
        - k8snode
        - env
        - system
        timeout: 2s
        override: false
      k8sattributes:
        extract:
          labels:
          - from: pod
            key: app.kubernetes.io/name
            tag_name: service.name
          - from: pod
            key: app.kubernetes.io/name
            tag_name: k8s.app.name
          - from: pod
            key: k8s-app
            tag_name: service.name
          - from: pod
            key: app.kubernetes.io/instance
            tag_name: k8s.app.instance
          - from: pod
            key: app.kubernetes.io/version
            tag_name: service.version
          - from: pod
            key: app.kubernetes.io/component
            tag_name: k8s.app.component
          - from: pod
            key: app.kubernetes.io/part-of
            tag_name: k8s.app.part_of
          - from: pod
            key: app.kubernetes.io/managed-by
            tag_name: k8s.app.managed_by
          # Extract all labels
          # - tag_name: $$1
          #   key_regex: (.*)
          #   from: pod
          # annotations:
          # Extract all annotations
          # - tag_name: $$1
          #   key_regex: (.*)
          #   from: pod
          metadata:
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.node.name
          - k8s.namespace.name
          - k8s.pod.start_time
          - k8s.replicaset.name
          - k8s.replicaset.uid
          - k8s.daemonset.name
          - k8s.daemonset.uid
          - k8s.job.name
          - k8s.job.uid
          - k8s.cronjob.name
          - k8s.statefulset.name
          - k8s.statefulset.uid
          - container.image.name
          - container.image.tag
          - k8s.container.name
          - container.id
          - container.image.name
          - container.image.tag
          - container.id
          - k8s.cluster.uid
        filter:
          node_from_env_var: K8S_NODE_NAME
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
          - from: resource_attribute
            name: k8s.node.name
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
        - sources:
          - from: connection
      # transform:
      #   trace_statements:
      #   - context: resource
      #     statements:
      #     - set(attributes["k8s.cluster.name"], "${K8S_CLUSTER_NAME}")
      #     - set(attributes["cluster"], "${K8S_CLUSTER_NAME}")
      tail_sampling:
        decision_wait: 10s
        num_traces: 100
        expected_new_traces_per_sec: 10
        policies:
        - name: error-policy
          type: status_code
          status_code:
            status_codes: [ERROR]
        - name: latency-policy
          type: latency
          latency:
            threshold_ms: 500

    exporters:
      otlphttp/gateway:
        endpoint: http://opentelemetry-collector-opentelemetry-gateway.opentelemetry.svc.cluster.local:4318

    connectors:
      datadog/connector:

    service:
      telemetry:
        logs:
          level: info
          encoding: json
        metrics:
          address: ${env:MY_POD_IP}:8888
      pipelines:
        logs: null
        metrics: null
        traces:
          receivers:
          - otlp
          processors:
          - resourcedetection
          - k8sattributes
          - memory_limiter
          - transform
          - batch
          exporters:
          - debug
          - otlphttp/gateway
        traces/sampling:
          receivers:
          - otlp
          processors:
          - resourcedetection
          - k8sattributes
          - memory_limiter
          - transform
          - batch
          exporters:
          - debug
          - otlphttp/gateway

  ports:
    otlp:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      protocol: TCP
      appProtocol: grpc
    otlp-http:
      enabled: true
      containerPort: 4318
      servicePort: 4318
      protocol: TCP
    jaeger-compact:
      enabled: false
    jaeger-thrift:
      enabled: false
    jaeger-grpc:
      enabled: false
    zipkin:
      enabled: false
    metrics:
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP

  service:
    enabled: true

  serviceMonitor:
    enabled: true
    extraLabels:
      prometheus.io/operator: portefaix

  prometheusRule:
    enabled: true
    groups: []
    defaultRules:
      enabled: false
    extraLabels:
      prometheus.io/operator: portefaix

opentelemetry-gateway:
  additionalLabels:
    app.kubernetes.io/part-of: opentelemetry-collector
    portefaix.xyz/version: v0.54.0

  mode: daemonset
  image:
    repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib

  useGOMEMLIMIT: true

  presets:
    logsCollection:
      enabled: false
    hostMetrics:
      enabled: false
    kubernetesAttributes:
      enabled: true
    kubeletMetrics:
      enabled: false
    kubernetesEvents:
      enabled: false
    clusterMetrics:
      enabled: false

  extraEnvsFrom:
  - secretRef:
      name: opentelemetry-datadog-credentials
  - secretRef:
      name: opentelemetry-lightstep-credentials
  - secretRef:
      name: opentelemetry-grafanacloud-credentials
  - secretRef:
      name: opentelemetry-kloudmate-credentials
  - secretRef:
      name: opentelemetry-hyperdx-credentials

  config:
    receivers:
      jaeger: null
      zipkin: null
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:4317
          http:
            endpoint: ${env:MY_POD_IP}:4318

    processors:
      batch:
        timeout: 10s
        send_batch_size: 1024
      batch/datadog:
        send_batch_max_size: 100
        send_batch_size: 10
        timeout: 10s
      # resource:
      #   attributes:
      #     - key: environment
      #       value: ${ENVIRONMENT}
      #       action: upsert
      #     - key: k8s.cluster.name
      #       value: ${CLUSTER_NAME}
      #       action: upsert
      #     - key: cluster
      #       value: ${CLUSTER_NAME}
      #       action: upsert
      #     - key: deployment.environment
      #       value: ${K8S_ENVIRONMENT_NAME}
      #       action: upsert
      #     - key: deployment.environment.name
      #       value: ${K8S_ENVIRONMENT_NAME}
      #       action: upsert
      transform:
        log_statements:
        - context: resource
          statements:
          - set(attributes["k8s.cluster.name"], "${K8S_CLUSTER_NAME}")
          - set(attributes["cluster"], "${K8S_CLUSTER_NAME}")
          - set(attributes["deployment.environment.name"], "${K8S_ENVIRONMENT_NAME}")
          # https://github.com/SigNoz/signoz/issues/6143
          - set(attributes["deployment.environment"], "${K8S_ENVIRONMENT_NAME}")
        metric_statements:
        - context: resource
          statements:
          - set(attributes["k8s.cluster.name"], "${K8S_CLUSTER_NAME}")
          - set(attributes["cluster"], "${K8S_CLUSTER_NAME}")
          - set(attributes["deployment.environment.name"], "${K8S_ENVIRONMENT_NAME}")
          # https://github.com/SigNoz/signoz/issues/6143
          - set(attributes["deployment.environment"], "${K8S_ENVIRONMENT_NAME}")
        trace_statements:
        - context: resource
          statements:
          - set(attributes["k8s.cluster.name"], "${K8S_CLUSTER_NAME}")
          - set(attributes["cluster"], "${K8S_CLUSTER_NAME}")
          - set(attributes["deployment.environment.name"], "${K8S_ENVIRONMENT_NAME}")
          # https://github.com/SigNoz/signoz/issues/6143
          - set(attributes["deployment.environment"], "${K8S_ENVIRONMENT_NAME}")

    extensions:
      basicauth/grafana_cloud:
        client_auth:
          username: "${env:GRAFANA_CLOUD_OTEL_COLLECTOR_ID}"
          password: "${env:GRAFANA_CLOUD_OTEL_COLLECTOR_TOKEN}"
      # bearertokenauth/dash0:
      #   scheme: Bearer
      #   token: ${env:DASH0_AUTHORIZATION_TOKEN}
      # opamp/lightstep:
      #   server:
      #     ws:
      #       endpoint: "wss://opamp.lightstep.com/v1/opamp"
      #       headers:
      #         Authorization: "bearer ${LS_OPAMP_API_KEY}"


    exporters:
    #   datadog:
    #     metrics:
    #       resource_attributes_as_tags: true
    #       histograms:
    #         mode: distributions
    #     api:
    #       key: ${env:DD_API_KEY}
    #       site: "datadoghq.com"
    #
      otlphttp/datadog:
        endpoint: https://api.datadoghq.com/api/intake/otlp/v1/metrics
        headers:
          dd-api-key: ${env:DD_API_KEY}
          dd-otel-metric-config: "{resource_attributes_as_tags: true}"
          dd-otlp-source: "datadoghq.com"

      otlphttp/grafana_cloud:
        endpoint: https://otlp-gateway-prod-eu-west-0.grafana.net/otlp
        auth:
          authenticator: basicauth/grafana_cloud

      otlp/honeycomb_logs:
        endpoint: "api.honeycomb.io:443"
        headers:
          "x-honeycomb-team": "${HONEYCOMB_API_KEY}"
          "x-honeycomb-dataset": "${HONEYCOMB_DATASET_LOGS}"
      otlp/honeycomb_metrics:
        endpoint: "api.honeycomb.io:443"
        headers:
          "x-honeycomb-team": "${HONEYCOMB_API_KEY}"
          "x-honeycomb-dataset": "${HONEYCOMB_DATASET_METRICS}"
      otlp/honeycomb_traces:
        endpoint: "api.honeycomb.io:443"
        headers:
          "x-honeycomb-team": "${HONEYCOMB_API_KEY}"
          "x-honeycomb-dataset": "${HONEYCOMB_DATASET_TRACES}"

      otlp/lightstep:
        endpoint: ingest.lightstep.com:443
        headers:
          "lightstep-access-token": "${LS_TOKEN}"

      otlphttp/hdx:
        endpoint: 'https://in-otel.hyperdx.io'
        headers:
          authorization: "${HYPERDX_API_KEY}"
        compression: gzip

      otlphttp/axiom_logs:
        compression: gzip
        endpoint: https://api.axiom.co
        headers:
          authorization: "Bearer {AXIOM_TOKEN}"
          x-axiom-dataset: "{AXIOM_DATASET_LOGS}"
      otlphttp/axiom_metrics:
        compression: gzip
        endpoint: https://api.axiom.co
        headers:
          authorization: "Bearer {AXIOM_TOKEN}"
          x-axiom-dataset: "{AXIOM_DATASET_METRICS}"
      otlphttp/axiom_traces:
        compression: gzip
        endpoint: https://api.axiom.co
        headers:
          authorization: "Bearer {AXIOM_TOKEN}"
          x-axiom-dataset: "{AXIOM_DATASET_TRACES}"

      # otlphttp/dash0:
      #   auth:
      #     authenticator: bearertokenauth/dash0
      #   endpoint: https://ingress.eu-west-1.aws.dash0.com
      # otlp/dash0:
      #   auth:
      #     authenticator: bearertokenauth/dash0
      #   endpoint: ingress.eu-west-1.aws.dash0.com:4317

      otlphttp/kloudmate:
        endpoint: https://otel.kloudmate.com:4318
        headers:
          Authorization: "${env:KLOUDMATE_API_KEY}"

      otlphttp/oneuptime:
        endpoint: "https://oneuptime.com/otlp"
        encoding: json
        headers:
          "Content-Type": "application/json"
          "x-oneuptime-token": "${ONEUPTIME_TOKEN}"

      # Endpoints in the cluster
      # ---------------------------

      otlphttp/loki:
        endpoint: http://loki-gateway.logging.svc.cluster.local:80/otlp
      # otlphttp/mimir:
      #   endpoint: http://mimir-gateway.monitoring.svc.cluster.local:80/otlp
      otlphttp/tempo:
        endpoint: http://tempo-distributor.tracing.svc.cluster.local:4318
        tls:
          insecure: true

      otlphttp/signoz:
        endpoint: http://signoz.observability.svc.cluster.local:80/otlp

    service:
      telemetry:
        logs:
          level: info
          encoding: json
        metrics:
          address: ${env:MY_POD_IP}:8888

      extensions:
      - health_check
      # - opamp/lightstep
      - basicauth/grafana_cloud

      pipelines:
        logs:
          receivers:
          - otlp
          processors:
          - memory_limiter
          # - resource
          - batch
          - batch/datadog
          exporters:
          - debug
          # - datadog
          - otlphttp/grafana_cloud
          # - otlp/honeycomb_logs
          # - otlp/lightstep
          # - otlphttp/dash0
          # - otlphttp/kloudmate
          - otlphttp/hdx
          - otlphttp/oneuptime
          # - otlphttp/axiom_logs
          - otlphttp/loki
          - otlphttp/signoz
        metrics:
          receivers:
          - otlp
          - prometheus
          processors:
          - memory_limiter
          # - resource
          - batch
          - batch/datadog
          exporters:
          - debug
          # - datadog
          - otlphttp/grafana_cloud
          # - otlp/honeycomb_metrics
          # - otlp/lightstep
          # - otlp/dash0
          # - otlphttp/dash0
          # - otlphttp/kloudmate
          - otlphttp/hdx
          - otlphttp/oneuptime
          - otlphttp/signoz
          # - otlphttp/axiom_metrics
        traces:
          receivers:
          - otlp
          processors:
          - memory_limiter
          # - resource
          - batch
          - batch/datadog
          exporters:
          - debug
          # - datadog
          - otlphttp/grafana_cloud
          # - otlp/honeycomb_traces
          # - otlp/lightstep
          # - otlp/dash0
          # - otlphttp/dash0
          # - otlphttp/kloudmate
          - otlphttp/hdx
          - otlphttp/oneuptime
          # - otlphttp/axiom_traces
          - otlphttp/tempo
          - otlphttp/signoz

  ports:
    otlp:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      protocol: TCP
      appProtocol: grpc
    otlp-http:
      enabled: true
      containerPort: 4318
      servicePort: 4318
      protocol: TCP
    jaeger-compact:
      enabled: false
    jaeger-thrift:
      enabled: false
    jaeger-grpc:
      enabled: false
    zipkin:
      enabled: false
    metrics:
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP

  service:
    enabled: true

  serviceMonitor:
    enabled: true
    extraLabels:
      prometheus.io/operator: portefaix

  prometheusRule:
    enabled: true
    groups:
    # Common
    - alert: OTelCollectorExporterFailedRequests
      expr: sum(rate(otelcol_exporter_send_failed_requests{}[1m])) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: Some exporter requests failed
        description: Maybe used destination has a problem or used payload is not correct
    defaultRules:
      enabled: true
    extraLabels:
      prometheus.io/operator: portefaix
