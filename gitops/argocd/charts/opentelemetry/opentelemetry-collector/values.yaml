---
# SPDX-FileCopyrightText: Copyright (C) Nicolas Lamirault <nicolas.lamirault@gmail.com>
# SPDX-License-Identifier: Apache-2.0

gateway:
  opentelemetry:
    enabled: true
    namespace: gateway-api
    logging:
      service: opentelemetry-collector-opentelemetry-logs
    tracing:
      service: opentelemetry-collector-opentelemetry-traces

grafanaDashboard:
  enabled: true
  grafanaOperator:
    enabled: true
    matchLabels:
      grafana.com/dashboards: portefaix

opentelemetry-logs:
  additionalLabels:
    app.kubernetes.io/part-of: opentelemetry-collector
    portefaix.xyz/version: v1.3.0

  mode: daemonset
  image:
    repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib

  useGOMEMLIMIT: true

  presets:
    logsCollection:
      enabled: true
      includeCollectorLogs: true
    hostMetrics:
      enabled: false
    kubernetesAttributes:
      enabled: true
    kubeletMetrics:
      enabled: false
    kubernetesEvents:
      enabled: false
    clusterMetrics:
      enabled: false

  config:
    receivers:
      jaeger: {}
      otlp:
        protocols:
          grpc:
            endpoint: "${env:MY_POD_IP}:4317"
          http:
            endpoint: "${env:MY_POD_IP}:4318"
            cors:
              allowed_origins:
              - "http://*"
              - "https://*"
      zipkin: {}
      filelog/k8s_pods:
        exclude: []
        include:
        - /var/log/pods/*/*/*.log
        max_concurrent_files: 200
        poll_interval: 1s
        max_log_size: 2MiB
        include_file_name: false
        include_file_path: true
        exclude_older_than: 24h
        start_at: end # beginning
        operators:
        - id: container-parser
          max_log_size: 102400
          type: container
        # Extract metadata from file path
        - type: regex_parser
          id: extract_metadata_from_filepath
          regex: '^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]+)\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$'
          parse_from: attributes["log.file.path"]
          output: add_cluster_name
          # Add cluster name attribute from environment variable
        - id: add_cluster_name
          type: add
          field: resource["k8s.cluster.name"]
          value: EXPR(env("K8S_CLUSTER_NAME"))
          output: move_stream
        retry_on_failure:
          enabled: true

    processors:
      attributes/pii_redaction:
        actions:
        - key: user.email
          action: delete
        - key: user.phone
          action: delete
      batch:
        send_batch_size: 8192
        timeout: 200ms
      filter/drop_debug_logs:
        logs:
          log_record:
          - severity_text: "DEBUG"
          # - 'attributes["level"] == "INFO"'
          # - severity_number < SEVERITY_NUMBER_INFO
      filter/sensitive:
        error_mode: ignore
        logs:
          log_record:
          - 'IsMatch(body, ".*(?i)password.*")' # Case insensitive match for any log line containing `password`
      resourcedetection:
        detectors:
        - k8snode
        - env
        - system
        timeout: 2s
        override: false
      k8sattributes:
        extract:
          labels:
          - from: pod
            key: app.kubernetes.io/instance
            tag_name: k8s.app.instance
          - from: pod
            key: app.kubernetes.io/component
            tag_name: k8s.app.component
          metadata:
          - container.image.tag
          - container.image.name
          - k8s.namespace.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.node.name
          - k8s.pod.start_time
          - k8s.deployment.name
          - k8s.replicaset.name
          - k8s.replicaset.uid
          - k8s.daemonset.name
          - k8s.daemonset.uid
          - k8s.job.name
          - k8s.job.uid
          - k8s.container.name
          - k8s.cronjob.name
          - k8s.statefulset.name
          - k8s.statefulset.uid
          - k8s.cluster.uid
          - service.namespace
          - service.name
          - service.version
          - service.instance.id
        filter:
          node_from_env_var: K8S_NODE_NAME
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
          - from: resource_attribute
            name: k8s.node.name
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
        - sources:
          - from: connection
      transform:
        log_statements:
        - context: resource
          statements:
          # - set(attributes["k8s.cluster.name"], "${K8S_CLUSTER_NAME}")
          # - set(attributes["cluster"], "${K8S_CLUSTER_NAME}")
          # - set(attributes["deployment.environment.name"], "${K8S_ENVIRONMENT_NAME}")
          - set(attributes["loki.attribute.labels"], "node, deployment, namespace, container, pod, app")
        - context: log
          statements:
          - set(log.time, Now())
          - set(log.observed_time, Now())
          - set(log.severity_text, "INFO") where IsMatch(log.severity_text, "")
          - set(log.severity_number, SEVERITY_NUMBER_INFO) where IsMatch(log.severity_text, "INFO") # https://opentelemetry.io/docs/specs/otel/logs/data-model/#field-severitynumber
        # Move trace context from attributes to the correct top-level fields
        - context: log
          statements:
          - set(trace_id.string, attributes["trace_id"])
          - set(span_id.string, attributes["span_id"])
          - set(flags, Int(attributes["trace_flags"]))
        # Delete the original, now redundant, trace context attributes
        - context: log
          statements:
          - delete_key(attributes, "trace_id")
          - delete_key(attributes, "span_id")
          - delete_key(attributes, "trace_flags")
        # Delete the duplicated resource attributes from the log record's attributes
        - context: log
          statements:
          - delete_key(attributes, "k8s.pod.name")
          - delete_key(attributes, "k8s.namespace.name")
          - delete_key(attributes, "k8s.container.name")
          - delete_key(attributes, "service.name")
        # Sensitive data
        - context: log
          statements:
          - replace_pattern(attributes["email"], "\\b([a-zA-Z0-9._%+-]{3})[a-zA-Z0-9._%+-]*@([a-zA-Z0-9]{2})[a-zA-Z0-9.-]*\\.(\\w{2,})\\b", "$1***@$2**.***")
          - replace_pattern(attributes["email"], "email=([^&]+)", Concat(["email=", SHA256("$1")], ""))
          - replace_pattern(attributes["phone"], "\\b(?:\\+?1[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?(\\d{4})\\b", "***-***-$1")
          - replace_pattern(attributes["address"], "\\b(\\d{1,5})\\s([\\w\\s]+),\\s([\\w\\s]+),\\s([A-Z]{2})\\s(\\d{5})\\b", "$1 **** REDACTED")
          - replace_pattern(attributes["ssn"], "\\b\\d{3}-\\d{2}-(\\d{4})\\b", "***-**-$1")

    exporters:
      otlphttp/gateway:
        endpoint: http://opentelemetry-collector-opentelemetry-gateway.opentelemetry.svc.cluster.local:4318

    service:
      telemetry:
        logs:
          level: info
          encoding: json
        metrics:
          address: ${env:MY_POD_IP}:8888
      pipelines:
        logs:
          receivers:
          - filelog/k8s_pods
          processors:
          # - attributes/pii_redaction
          # - filter/drop_debug_logs
          - resourcedetection
          - k8sattributes
          - filter/sensitive
          - memory_limiter
          - transform
          - batch
          exporters:
          # https://artifacthub.io/packages/helm/opentelemetry-helm/opentelemetry-collector#warning-warning-risk-of-looping-the-exported-logs-back-into-the-receiver-causing-log-explosion
          # - debug
          - otlphttp/gateway
        metrics: {}
        traces: {}

  ports:
    otlp:
      enabled: false
      containerPort: 4317
      servicePort: 4317
      protocol: TCP
      appProtocol: grpc
    otlp-http:
      enabled: false
      containerPort: 4318
      servicePort: 4318
      protocol: TCP
    jaeger-compact:
      enabled: false
    jaeger-thrift:
      enabled: false
    jaeger-grpc:
      enabled: false
    zipkin:
      enabled: false
    metrics:
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP

  securityContext:
    runAsUser: 0
    runAsGroup: 0

  service:
    enabled: true

  serviceMonitor:
    enabled: true
    extraLabels:
      prometheus.io/operator: portefaix

  prometheusRule:
    enabled: true
    groups: []
    defaultRules:
      enabled: false
    extraLabels:
      prometheus.io/operator: portefaix

opentelemetry-metrics:
  additionalLabels:
    app.kubernetes.io/part-of: opentelemetry-collector
    portefaix.xyz/version: v1.3.0

  mode: daemonset
  image:
    repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib

  useGOMEMLIMIT: true

  presets:
    logsCollection:
      enabled: false
    hostMetrics:
      enabled: true
    kubernetesAttributes:
      enabled: true
    kubeletMetrics:
      enabled: true
    kubernetesEvents:
      enabled: false
    clusterMetrics:
      enabled: false

  config:
    receivers:
      # https://github.com/helm/helm/pull/12879
      jaeger: null
      otlp: null
      zipkin: null
      kubeletstats:
        insecure_skip_verify: true
        metric_groups:
        - container
        - node
        - pod
        - volume
        metrics:
          container.cpu.usage:
            enabled: true
          container.uptime:
            enabled: true
          k8s.container.cpu_limit_utilization:
            enabled: true
          k8s.container.cpu_request_utilization:
            enabled: true
          k8s.container.memory_limit_utilization:
            enabled: true
          k8s.container.memory_request_utilization:
            enabled: true
          k8s.node.cpu.usage:
            enabled: true
          k8s.node.uptime:
            enabled: true
          k8s.pod.cpu.usage:
            enabled: true
          k8s.pod.cpu_limit_utilization:
            enabled: true
          k8s.pod.cpu_request_utilization:
            enabled: true
          k8s.pod.memory_limit_utilization:
            enabled: true
          k8s.pod.memory_request_utilization:
            enabled: true
          k8s.pod.uptime:
            enabled: true
        node: ${env:K8S_NODE_NAME}
      hostmetrics:
        scrapers:
          cpu:
            metrics:
              system.cpu.logical.count:
                enabled: true
          memory:
            metrics:
              system.memory.utilization:
                enabled: true
              system.memory.limit:
                enabled: true
          load: {}
          disk: {}
          filesystem:
            exclude_fs_types:
              fs_types:
              - autofs
              - binfmt_misc
              - bpf
              - cgroup2
              - configfs
              - debugfs
              - devpts
              - devtmpfs
              - fusectl
              - hugetlbfs
              - iso9660
              - mqueue
              - nsfs
              - overlay
              - proc
              - procfs
              - pstore
              - rpc_pipefs
              - securityfs
              - selinuxfs
              - squashfs
              - sysfs
              - tracefs
              match_type: strict
            exclude_mount_points:
              match_type: regexp
              mount_points:
              - /dev/*
              - /proc/*
              - /sys/*
              - /run/k3s/containerd/*
              - /var/lib/docker/*
              - /var/lib/kubelet/*
              - /snap/*
            metrics:
              system.filesystem.utilization:
                enabled: true
          network:
            exclude:
              interfaces:
              - ^veth.*$
              - ^docker.*$
              - ^br-.*$
              - ^flannel.*$
              - ^cali.*$
              - ^cbr.*$
              - ^cni.*$
              - ^dummy.*$
              - ^tailscale.*$
              - ^lo$
              match_type: regexp
          paging: {}
          processes: {}
          process: {}
    # mute_process_user_error: true
    # metrics:
    #   process.cpu.utilization:
    #     enabled: true
    #   process.memory.utilization:
    #     enabled: true
    #   process.threads:
    #     enabled: true
    #   process.paging.faults:
    #     enabled: true

    processors:
      batch:
        send_batch_size: 8192
        timeout: 200ms
      resourcedetection:
        detectors:
        - k8snode
        - env
        - system
        timeout: 2s
        override: false
      k8sattributes:
        extract:
          labels:
          - from: pod
            key: app.kubernetes.io/name
            tag_name: service.name
          - from: pod
            key: app.kubernetes.io/name
            tag_name: k8s.app.name
          - from: pod
            key: k8s-app
            tag_name: service.name
          - from: pod
            key: app.kubernetes.io/instance
            tag_name: k8s.app.instance
          - from: pod
            key: app.kubernetes.io/version
            tag_name: service.version
          - from: pod
            key: app.kubernetes.io/component
            tag_name: k8s.app.component
          - from: pod
            key: app.kubernetes.io/part-of
            tag_name: k8s.app.part_of
          - from: pod
            key: app.kubernetes.io/managed-by
            tag_name: k8s.app.managed_by
          # Extract all labels
          # - tag_name: $$1
          #   key_regex: (.*)
          #   from: pod
          # annotations:
          # Extract all annotations
          # - tag_name: $$1
          #   key_regex: (.*)
          #   from: pod
          metadata:
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.node.name
          - k8s.namespace.name
          - k8s.pod.start_time
          - k8s.replicaset.name
          - k8s.replicaset.uid
          - k8s.daemonset.name
          - k8s.daemonset.uid
          - k8s.job.name
          - k8s.job.uid
          - k8s.cronjob.name
          - k8s.statefulset.name
          - k8s.statefulset.uid
          - container.image.name
          - container.image.tag
          - k8s.container.name
          - container.id
          - container.image.name
          - container.image.tag
          - container.id
          - k8s.cluster.uid
        filter:
          node_from_env_var: K8S_NODE_NAME
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
          - from: resource_attribute
            name: k8s.node.name
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
        - sources:
          - from: connection
      # transform:
      #   metric_statements:
      #   - context: resource
      #     statements:
      #     - set(attributes["k8s.cluster.name"], "${K8S_CLUSTER_NAME}")
      #     - set(attributes["cluster"], "${K8S_CLUSTER_NAME}")
      #     - set(attributes["deployment.environment.name"], "${K8S_ENVIRONMENT_NAME}")

    exporters:
      otlphttp/gateway:
        endpoint: http://opentelemetry-collector-opentelemetry-gateway.opentelemetry.svc.cluster.local:4318

    service:
      telemetry:
        logs:
          level: info
          encoding: json
        metrics:
          address: ${env:MY_POD_IP}:8888
      pipelines:
        logs: null
        metrics:
          receivers:
          # - otlp
          - prometheus
          processors:
          - resourcedetection
          - k8sattributes
          - memory_limiter
          - transform
          - batch
          exporters:
          - debug
          - otlphttp/gateway
        traces: null

  ports:
    otlp:
      enabled: false
      containerPort: 4317
      servicePort: 4317
      protocol: TCP
      appProtocol: grpc
    otlp-http:
      enabled: false
      containerPort: 4318
      servicePort: 4318
      protocol: TCP
    jaeger-compact:
      enabled: false
    jaeger-thrift:
      enabled: false
    jaeger-grpc:
      enabled: false
    zipkin:
      enabled: false
    metrics:
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP

  service:
    enabled: true

  serviceMonitor:
    enabled: true
    extraLabels:
      prometheus.io/operator: portefaix

  prometheusRule:
    enabled: true
    groups: []
    defaultRules:
      enabled: false
    extraLabels:
      prometheus.io/operator: portefaix

opentelemetry-metrics-cluster:
  additionalLabels:
    app.kubernetes.io/part-of: opentelemetry-collector
    portefaix.xyz/version: v1.3.0

  mode: deployment
  image:
    repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib

  useGOMEMLIMIT: true

  presets:
    logsCollection:
      enabled: false
    hostMetrics:
      enabled: false
    kubernetesAttributes:
      enabled: true
    kubeletMetrics:
      enabled: false
    kubernetesEvents:
      enabled: true
    clusterMetrics:
      enabled: true

  config:
    receivers:
      jaeger: null
      otlp: null
      zipkin: null
      k8s_cluster:
        collection_interval: 30s
        allocatable_types_to_report:
        - cpu
        - memory
        - storage
        node_conditions_to_report:
        - Ready
        - MemoryPressure
        - DiskPressure
        - NetworkUnavailable
        distribution: kubernetes
        metrics:
          k8s.node.condition:
            enabled: true
          k8s.pod.status_reason:
            enabled: true
      k8sobjects:
        objects:
        - exclude_watch_type:
          - DELETED
          group: events.k8s.io
          mode: watch
          name: events

    processors:
      batch:
        send_batch_size: 8192
        timeout: 200ms
      resourcedetection:
        detectors:
        - k8snode
        - env
        - system
        timeout: 2s
        override: false
      k8sattributes:
        extract:
          labels:
          - from: pod
            key: app.kubernetes.io/name
            tag_name: service.name
          - from: pod
            key: app.kubernetes.io/name
            tag_name: k8s.app.name
          - from: pod
            key: k8s-app
            tag_name: service.name
          - from: pod
            key: app.kubernetes.io/instance
            tag_name: k8s.app.instance
          - from: pod
            key: app.kubernetes.io/version
            tag_name: service.version
          - from: pod
            key: app.kubernetes.io/component
            tag_name: k8s.app.component
          - from: pod
            key: app.kubernetes.io/part-of
            tag_name: k8s.app.part_of
          - from: pod
            key: app.kubernetes.io/managed-by
            tag_name: k8s.app.managed_by
          # Extract all labels
          # - tag_name: $$1
          #   key_regex: (.*)
          #   from: pod
          # annotations:
          # Extract all annotations
          # - tag_name: $$1
          #   key_regex: (.*)
          #   from: pod
          metadata:
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.node.name
          - k8s.namespace.name
          - k8s.pod.start_time
          - k8s.replicaset.name
          - k8s.replicaset.uid
          - k8s.daemonset.name
          - k8s.daemonset.uid
          - k8s.job.name
          - k8s.job.uid
          - k8s.cronjob.name
          - k8s.statefulset.name
          - k8s.statefulset.uid
          - container.image.name
          - container.image.tag
          - k8s.container.name
          - container.id
          - container.image.name
          - container.image.tag
          - container.id
          - k8s.cluster.uid
        filter:
          node_from_env_var: K8S_NODE_NAME
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
          - from: resource_attribute
            name: k8s.node.name
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
        - sources:
          - from: connection
      # transform:
      #   metric_statements:
      #   - context: resource
      #     statements:
      #     - set(attributes["k8s.cluster.name"], "${K8S_CLUSTER_NAME}")
      #     - set(attributes["cluster"], "${K8S_CLUSTER_NAME}")
      #     - set(attributes["deployment.environment.name"], "${K8S_ENVIRONMENT_NAME}")

    exporters:
      otlphttp/gateway:
        endpoint: http://opentelemetry-collector-opentelemetry-gateway.opentelemetry.svc.cluster.local:4318

    service:
      telemetry:
        logs:
          level: info
          encoding: json
        metrics:
          address: ${env:MY_POD_IP}:8888
      pipelines:
        logs:
          receivers:
          - k8sobjects
          processors:
          - resourcedetection
          - k8sattributes
          - memory_limiter
          - batch
          exporters:
          - debug
          - otlphttp/gateway
        metrics:
          receivers:
          # - otlp
          - prometheus
          - k8s_cluster
          processors:
          - resourcedetection
          - k8sattributes
          - memory_limiter
          # - transform
          - batch
          exporters:
          - debug
          - otlphttp/gateway
        traces: null

  ports:
    otlp:
      enabled: false
      containerPort: 4317
      servicePort: 4317
      protocol: TCP
      appProtocol: grpc
    otlp-http:
      enabled: false
      containerPort: 4318
      servicePort: 4318
      protocol: TCP
    jaeger-compact:
      enabled: false
    jaeger-thrift:
      enabled: false
    jaeger-grpc:
      enabled: false
    zipkin:
      enabled: false
    metrics:
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP

  service:
    enabled: true

  serviceMonitor:
    enabled: true
    extraLabels:
      prometheus.io/operator: portefaix

  prometheusRule:
    enabled: true
    groups: []
    defaultRules:
      enabled: true
    extraLabels:
      prometheus.io/operator: portefaix

opentelemetry-traces:
  additionalLabels:
    app.kubernetes.io/part-of: opentelemetry-collector
    portefaix.xyz/version: v1.3.0

  mode: deployment
  image:
    repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib

  useGOMEMLIMIT: true

  presets:
    logsCollection:
      enabled: false
    hostMetrics:
      enabled: false
    kubernetesAttributes:
      enabled: true
    kubeletMetrics:
      enabled: false
    kubernetesEvents:
      enabled: false
    clusterMetrics:
      enabled: false

  config:
    receivers:
      jaeger: null
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      zipkin: null

    processors:
      batch:
        send_batch_size: 8192
        timeout: 200ms

      # attributes/pii_redaction:
      #   actions:
      #     - key: user.email
      #       action: delete
      #     - key: user.phone
      #       action: delete

      redaction:
        # summary: debug
        allow_all_keys: true
        # allowed_keys:
        # - http.method
        # - http.url
        # - http.status_code
        # Mask any attribute key that matches these regex patterns
        blocked_key_patterns:
        - .*token.*
        - .*password.*
        - user.email
        blocked_values:
        # This regex detects Visa, Mastercard, and American Express numbers,
        # even when they include spaces or dashes as separators.
        - \b(?:4[0-9]{3}|5[1-5][0-9]{2}|3[47][0-9]{2})[ -]?([0-9]{4})[ -]?([0-9]{4})[ -]?([0-9]{4})\b
        # Block anything that looks like an email RFC 5322-ish
        - '[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
        allowed_values:
        - '.+@portefaix\.xyz'
        url_sanitizer:
          enabled: true
          attributes: ["http.url", "url"]
        hash_function: sha3

      transform/standardize_fields:
        error_mode: ignore
        trace_statements:
        - context: span
          statements:
          - rename(attributes["user_id"], "user.id")
      transform/redact_sensitive:
        error_mode: ignore
        trace_statements:
        - replace_pattern(span.attributes["http.url"], "email=[^&]+", "email=[REDACTED]")

      # transform:
      #   trace_statements:
      #   - context: resource
      #     statements:
      #     - set(attributes["k8s.cluster.name"], "${K8S_CLUSTER_NAME}")
      #     - set(attributes["cluster"], "${K8S_CLUSTER_NAME}")

      resourcedetection:
        detectors:
        - k8snode
        - env
        - system
        timeout: 2s
        override: false

      k8sattributes:
        extract:
          labels:
          - from: pod
            key: app.kubernetes.io/name
            tag_name: service.name
          - from: pod
            key: app.kubernetes.io/name
            tag_name: k8s.app.name
          - from: pod
            key: k8s-app
            tag_name: service.name
          - from: pod
            key: app.kubernetes.io/instance
            tag_name: k8s.app.instance
          - from: pod
            key: app.kubernetes.io/version
            tag_name: service.version
          - from: pod
            key: app.kubernetes.io/component
            tag_name: k8s.app.component
          - from: pod
            key: app.kubernetes.io/part-of
            tag_name: k8s.app.part_of
          - from: pod
            key: app.kubernetes.io/managed-by
            tag_name: k8s.app.managed_by
          # Extract all labels
          # - tag_name: $$1
          #   key_regex: (.*)
          #   from: pod
          # annotations:
          # Extract all annotations
          # - tag_name: $$1
          #   key_regex: (.*)
          #   from: pod
          metadata:
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.node.name
          - k8s.namespace.name
          - k8s.pod.start_time
          - k8s.replicaset.name
          - k8s.replicaset.uid
          - k8s.daemonset.name
          - k8s.daemonset.uid
          - k8s.job.name
          - k8s.job.uid
          - k8s.cronjob.name
          - k8s.statefulset.name
          - k8s.statefulset.uid
          - container.image.name
          - container.image.tag
          - k8s.container.name
          - container.id
          - container.image.name
          - container.image.tag
          - container.id
          - k8s.cluster.uid
        filter:
          node_from_env_var: K8S_NODE_NAME
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
          - from: resource_attribute
            name: k8s.node.name
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
        - sources:
          - from: connection

      filter/healthCheck:
        error_mode: ignore
        traces:
          span:
          - 'attributes["http.target"] == "/health"'
          # - 'attributes["http.request.method"] == nil'

    exporters:
      otlphttp/gateway:
        endpoint: http://opentelemetry-collector-opentelemetry-gateway.opentelemetry.svc.cluster.local:4318

    connectors:
      datadog/connector:

    service:
      telemetry:
        logs:
          level: info
          encoding: json
        metrics:
          address: ${env:MY_POD_IP}:8888
      pipelines:
        logs: null
        metrics: null
        traces:
          receivers:
          - otlp
          processors:
          # - attributes/pii_redaction
          - resourcedetection
          - k8sattributes
          - memory_limiter
          - filter/healthCheck
          - redaction
          - transform/standardize_fields
          - transform/redact_sensitive
          # - transform
          - batch
          exporters:
          - debug
          - otlphttp/gateway
        traces/sampling:
          receivers:
          - otlp
          processors:
          - resourcedetection
          - k8sattributes
          - memory_limiter
          - transform
          - batch
          exporters:
          - debug
          - otlphttp/gateway

  ports:
    otlp:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      protocol: TCP
      appProtocol: grpc
    otlp-http:
      enabled: true
      containerPort: 4318
      servicePort: 4318
      protocol: TCP
    jaeger-compact:
      enabled: false
    jaeger-thrift:
      enabled: false
    jaeger-grpc:
      enabled: false
    zipkin:
      enabled: false
    metrics:
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP

  service:
    enabled: true

  serviceMonitor:
    enabled: true
    extraLabels:
      prometheus.io/operator: portefaix

  prometheusRule:
    enabled: true
    groups: []
    defaultRules:
      enabled: false
    extraLabels:
      prometheus.io/operator: portefaix

opentelemetry-gateway:
  additionalLabels:
    app.kubernetes.io/part-of: opentelemetry-collector
    portefaix.xyz/version: v1.3.0

  mode: daemonset
  image:
    repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib

  useGOMEMLIMIT: true

  presets:
    logsCollection:
      enabled: false
    hostMetrics:
      enabled: false
    kubernetesAttributes:
      enabled: true
    kubeletMetrics:
      enabled: false
    kubernetesEvents:
      enabled: false
    clusterMetrics:
      enabled: false

  extraEnvsFrom:
  - secretRef:
      name: opentelemetry-datadog-credentials
  - secretRef:
      name: opentelemetry-lightstep-credentials
  - secretRef:
      name: opentelemetry-grafanacloud-credentials
  - secretRef:
      name: opentelemetry-kloudmate-credentials
  - secretRef:
      name: opentelemetry-hyperdx-credentials
  - secretRef:
      name: opentelemetry-cardinalhq-credentials

  config:
    receivers:
      jaeger: null
      zipkin: null
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:4317
          http:
            endpoint: ${env:MY_POD_IP}:4318

    processors:
      batch:
        send_batch_size: 8192
        timeout: 200ms
      batch/datadog:
        send_batch_max_size: 100
        send_batch_size: 10
        timeout: 10s
      transform:
        log_statements:
        - context: resource
          statements:
          - set(attributes["k8s.cluster.name"], "${K8S_CLUSTER_NAME}")
          - set(attributes["cluster"], "${K8S_CLUSTER_NAME}")
          - set(attributes["deployment.environment.name"], "${K8S_ENVIRONMENT_NAME}")
          # https://github.com/SigNoz/signoz/issues/6143
          - set(attributes["deployment.environment"], "${K8S_ENVIRONMENT_NAME}")
        metric_statements:
        - context: resource
          statements:
          - set(attributes["k8s.cluster.name"], "${K8S_CLUSTER_NAME}")
          - set(attributes["cluster"], "${K8S_CLUSTER_NAME}")
          - set(attributes["deployment.environment.name"], "${K8S_ENVIRONMENT_NAME}")
          # https://github.com/SigNoz/signoz/issues/6143
          - set(attributes["deployment.environment"], "${K8S_ENVIRONMENT_NAME}")
        trace_statements:
        - context: resource
          statements:
          - set(attributes["k8s.cluster.name"], "${K8S_CLUSTER_NAME}")
          - set(attributes["cluster"], "${K8S_CLUSTER_NAME}")
          - set(attributes["deployment.environment.name"], "${K8S_ENVIRONMENT_NAME}")
          # https://github.com/SigNoz/signoz/issues/6143
          - set(attributes["deployment.environment"], "${K8S_ENVIRONMENT_NAME}")

    extensions:
      basicauth/grafana_cloud:
        client_auth:
          username: "${env:GRAFANA_CLOUD_OTEL_COLLECTOR_ID}"
          password: "${env:GRAFANA_CLOUD_OTEL_COLLECTOR_TOKEN}"
      # bearertokenauth/dash0:
      #   scheme: Bearer
      #   token: ${env:DASH0_AUTHORIZATION_TOKEN}
      # opamp/lightstep:
      #   server:
      #     ws:
      #       endpoint: "wss://opamp.lightstep.com/v1/opamp"
      #       headers:
      #         Authorization: "bearer ${LS_OPAMP_API_KEY}"
      basicauth/greptimedbcloud:
        client_auth:
          username: "${env:GREPTIME_CLOUD_USERNAME}"
          password: "${env:GREPTIME_CLOUD_PASSWORD}"

    connectors:
      # count/log_errors:
      #   logs:
      #     log_error.count:
      #       description: count of errors logged
      #       conditions:
      #         - severity_number >= SEVERITY_NUMBER_ERROR

      # https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/routingconnector/README.md
      routing/logs:
        default_pipelines:
        - logs/default
        error_mode: ignore
        table:
        - context: resource
          condition: |
            resource.attributes["service.namespace"] == "gitops"' or
            resource.attributes["service.namespace"] == "logging"' or
            resource.attributes["service.namespace"] == "monitoring"' or
            resource.attributes["service.namespace"] == "observability"'
            resource.attributes["service.namespace"] == "opentelemetry"'
          pipelines:
          - logs/saas
          - logs/local
        - context: resource
          condition: |
            resource.attributes["service.namespace"] != "default"'
          pipelines:
          - logs/local

      routing/metrics:
        default_pipelines:
        - metrics/default
        error_mode: ignore
        table:
        - context: resource
          condition: |
            resource.attributes["service.namespace"] == "gitops"' or
            resource.attributes["service.namespace"] == "kube-system"' or
            resource.attributes["service.namespace"] == "logging"' or
            resource.attributes["service.namespace"] == "monitoring"' or
            resource.attributes["service.namespace"] == "observability"'
            resource.attributes["service.namespace"] == "opentelemetry"'
          pipelines:
          - metrics/saas
          - metrics/local
        - context: resource
          condition: |
            resource.attributes["service.namespace"] != "default"'
          pipelines:
          - metrics/local

      routing/traces:
        default_pipelines:
        - traces/default
        error_mode: ignore
        table:
        - context: resource
          condition: |
            resource.attributes["service.namespace"] == "gitops"' or
            resource.attributes["service.namespace"] == "kube-system"' or
            resource.attributes["service.namespace"] == "observability"'
          pipelines:
          - traces/saas
          - traces/local
        - context: resource
          condition: |
            resource.attributes["service.namespace"] != "default"'
          pipelines:
          - traces/local

    exporters:
      datadog:
        api:
          key: ${env:DD_API_KEY}

      otlphttp/grafanacloud:
        endpoint: https://otlp-gateway-prod-eu-west-0.grafana.net/otlp
        auth:
          authenticator: basicauth/grafana_cloud

      otlp/honeycomblogs:
        endpoint: "api.honeycomb.io:443"
        headers:
          "x-honeycomb-team": "${HONEYCOMB_API_KEY}"
          "x-honeycomb-dataset": "${HONEYCOMB_DATASET_LOGS}"
      otlp/honeycombmetrics:
        endpoint: "api.honeycomb.io:443"
        headers:
          "x-honeycomb-team": "${HONEYCOMB_API_KEY}"
          "x-honeycomb-dataset": "${HONEYCOMB_DATASET_METRICS}"
      otlp/honeycombtraces:
        endpoint: "api.honeycomb.io:443"
        headers:
          "x-honeycomb-team": "${HONEYCOMB_API_KEY}"
          "x-honeycomb-dataset": "${HONEYCOMB_DATASET_TRACES}"

      otlp/lightstep:
        endpoint: ingest.lightstep.com:443
        headers:
          "lightstep-access-token": "${LS_TOKEN}"

      otlphttp/hdx:
        endpoint: "https://in-otel.hyperdx.io"
        headers:
          authorization: "${HYPERDX_API_KEY}"
        compression: gzip

      otlphttp/axiomlogs:
        compression: gzip
        endpoint: https://api.axiom.co
        headers:
          authorization: "Bearer {AXIOM_TOKEN}"
          x-axiom-dataset: "{AXIOM_DATASET_LOGS}"
      otlphttp/axiommetrics:
        compression: gzip
        endpoint: https://api.axiom.co
        headers:
          authorization: "Bearer {AXIOM_TOKEN}"
          x-axiom-dataset: "{AXIOM_DATASET_METRICS}"
      otlphttp/axiomtraces:
        compression: gzip
        endpoint: https://api.axiom.co
        headers:
          authorization: "Bearer {AXIOM_TOKEN}"
          x-axiom-dataset: "{AXIOM_DATASET_TRACES}"

      # otlphttp/dash0:
      #   auth:
      #     authenticator: bearertokenauth/dash0
      #   endpoint: https://ingress.eu-west-1.aws.dash0.com
      # otlp/dash0:
      #   auth:
      #     authenticator: bearertokenauth/dash0
      #   endpoint: ingress.eu-west-1.aws.dash0.com:4317

      otlphttp/kloudmate:
        endpoint: https://otel.kloudmate.com:4318
        headers:
          Authorization: "${env:KLOUDMATE_API_KEY}"

      otlphttp/oneuptime:
        endpoint: "https://oneuptime.com/otlp"
        encoding: json
        headers:
          "Content-Type": "application/json"
          "x-oneuptime-token": "${ONEUPTIME_TOKEN}"

      otlphttp/cardinal:
        endpoint: https://otelhttp.intake.us-east-2.aws.cardinalhq.io
        headers:
          x-cardinalhq-api-key: "${CARDINALHQ_API_KEY}"

      otlphttp/greptimedbcloudlogs:
        endpoint: "${env:GREPTIME_CLOUD_ENDPOINT_OTLP}"
        auth:
          authenticator: basicauth/greptimedbcloud
        headers:
          x-greptime-db-name: "${env:GREPTIME_CLOUD_DB}"
          x-greptime-log-table-name: "opentelemetry_logs"
          x-greptime-pipeline-name: "greptime_logs_v1"
          x-greptime-pipeline-version: "v1.0.0"
          x-greptime-log-extract-keys: k8s_container_name,k8s_namespace_name,k8s_pod_name,podclusterid,type,container.image.name,container.image.tag
      otlphttp/greptimedbcloudmetrics:
        endpoint: "${env:GREPTIME_CLOUD_ENDPOINT_OTLP}"
        auth:
          authenticator: basicauth/greptimedbcloud
        headers:
          x-greptime-db-name: "${env:GREPTIME_CLOUD_DB}"
      otlphttp/greptimedbcloudtraces:
        endpoint: "${env:GREPTIME_CLOUD_ENDPOINT_OTLP}"
        auth:
          authenticator: basicauth/greptimedbcloud
        headers:
          x-greptime-db-name: "${env:GREPTIME_CLOUD_DB}"
          x-greptime-pipeline-name: "greptime_trace_v1"

      # Endpoints in the cluster
      # ---------------------------

      otlphttp/loki:
        endpoint: http://loki-gateway.logging.svc.cluster.local:80/otlp

      # otlphttp/mimir:
      #   endpoint: http://mimir-gateway.monitoring.svc.cluster.local:80/otlp

      otlphttp/tempo:
        endpoint: http://tempo-distributor.tracing.svc.cluster.local:4318
        tls:
          insecure: true

      otlphttp/signoz:
        endpoint: http://signoz.observability.svc.cluster.local:80/otlp

      otlphttp/greptimedblogs:
        endpoint: "http://127.0.0.1:4000/v1/otlp"
        headers:
          x-greptime-db-name: "portefaix"
          x-greptime-log-table-name: "opentelemetry_logs"
          x-greptime-pipeline-name: "greptime_logs_v1"
        tls:
          insecure: true
      otlphttp/greptimedbmetrics:
        endpoint: "http://127.0.0.1:4000/v1/otlp"
        headers:
          x-greptime-db-name: "portefaix"
        tls:
          insecure: true
      otlphttp/greptimedbtraces:
        endpoint: "http://127.0.0.1:4000/v1/otlp"
        headers:
          x-greptime-db-name: "portefaix"
          x-greptime-pipeline-name: "greptime_trace_v1"
        tls:
          insecure: true

    service:
      telemetry:
        logs:
          level: info
          encoding: json
        metrics:
          address: ${env:MY_POD_IP}:8888

      extensions:
      - health_check
      # - opamp/lightstep
      - basicauth/grafana_cloud

      pipelines:

        # OpenTelemetry / Logs
        logs/in:
          receivers:
          - otlp
          processors:
          - memory_limiter
          - batch
          exporters:
          - routing/logs

        logs/default:
          receivers:
          - routing/logs
          processors:
          - memory_limiter
          - batch
          - batch/datadog
          exporters:
          - debug

        logs/local:
          receivers:
          - routing/logs
          processors:
          - memory_limiter
          - batch
          exporters:
          - otlphttp/loki
          - otlphttp/signoz
          - otlphttp/greptimedblogs

        logs/saas:
          receivers:
          - routing/logs
          processors:
          - memory_limiter
          - batch
          - batch/datadog
          exporters:
          - datadog
          - otlphttp/grafanacloud
          # - otlp/honeycomblogs
          # - otlp/lightstep
          # - otlphttp/dash0
          # - otlphttp/kloudmate
          - otlphttp/hdx
          - otlphttp/oneuptime
          # - otlphttp/axiomlogs
          - otlphttp/cardinal
          - otlphttp/greptimedbcloudlogs

        # OpenTelemetry / Metrics

        metrics/in:
          receivers:
          - otlp
          - prometheus
          processors:
          - memory_limiter
          - batch
          exporters:
          - routing/metrics

        metrics/default:
          receivers:
          - routing/metrics
          processors:
          - memory_limiter
          - batch
          exporters:
          - debug

        metrics/local:
          receivers:
          - routing/metrics
          processors:
          - memory_limiter
          - batch
          exporters:
          - otlphttp/signoz
          - otlphttp/greptimedbmetrics

          metrics/saas:
            receivers:
            - routing/metrics
            processors:
            - memory_limiter
            - batch
            - batch/datadog
            exporters:
            - datadog
            - otlphttp/grafanacloud
            # - otlp/honeycombmetrics
            # - otlp/lightstep
            # - otlp/dash0
            # - otlphttp/dash0
            # - otlphttp/kloudmate
            - otlphttp/hdx
            - otlphttp/oneuptime
            - otlphttp/cardinal
            # - otlphttp/axiommetrics
            - otlphttp/greptimedbcloudmetrics

        # OpenTelemetry / Traces

        traces/in:
          receivers:
          - otlp
          processors:
          - memory_limiter
          - batch
          exporters:
          - routing/traces

        traces/default:
          receivers:
          - routing/traces
          processors:
          - memory_limiter
          - batch
          exporters:
          - debug

        traces/local:
          receivers:
          - routing/traces
          processors:
          - memory_limiter
          - batch
          - batch/datadog
          exporters:
          - otlphttp/tempo
          - otlphttp/signoz
          - otlphttp/greptimedbtraces

        traces/saas:
          receivers:
          - routing/traces
          processors:
          - memory_limiter
          - batch
          - batch/datadog
          exporters:
          - datadog
          - otlphttp/grafanacloud
          # - otlp/honeycombtraces
          # - otlp/lightstep
          # - otlp/dash0
          # - otlphttp/dash0
          # - otlphttp/kloudmate
          - otlphttp/hdx
          - otlphttp/oneuptime
          # - otlphttp/axiomtraces
          - otlphttp/cardinal
          - otlphttp/greptimedbcloudtraces

  ports:
    otlp:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      protocol: TCP
      appProtocol: grpc
    otlp-http:
      enabled: true
      containerPort: 4318
      servicePort: 4318
      protocol: TCP
    jaeger-compact:
      enabled: false
    jaeger-thrift:
      enabled: false
    jaeger-grpc:
      enabled: false
    zipkin:
      enabled: false
    metrics:
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP

  service:
    enabled: true

  serviceMonitor:
    enabled: true
    extraLabels:
      prometheus.io/operator: portefaix

  prometheusRule:
    enabled: true
    groups:
    # Common
    - alert: OTelCollectorExporterFailedRequests
      expr: sum(rate(otelcol_exporter_send_failed_requests{}[1m])) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: Some exporter requests failed
        description: Maybe used destination has a problem or used payload is not correct
    defaultRules:
      enabled: true
    extraLabels:
      prometheus.io/operator: portefaix

opentelemetry-ai:
  additionalLabels:
    app.kubernetes.io/part-of: opentelemetry-collector
    portefaix.xyz/version: v1.3.0

  mode: deployment
  image:
    repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib

  useGOMEMLIMIT: true

  presets:
    logsCollection:
      enabled: false
    hostMetrics:
      enabled: false
    kubernetesAttributes:
      enabled: true
    kubeletMetrics:
      enabled: false
    kubernetesEvents:
      enabled: false
    clusterMetrics:
      enabled: false

  config:
    receivers:
      jaeger: null
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      zipkin: null

    processors:
      batch:
        send_batch_size: 8192
        timeout: 200ms
      memory_limiter:
        # 80% of maximum memory up to 2G
        limit_mib: 1500
        # 25% of limit up to 2G
        spike_limit_mib: 512
        check_interval: 5s
      resourcedetection:
        detectors:
        - k8snode
        - env
        - system
        timeout: 2s
        override: false
      k8sattributes:
        extract:
          labels:
          - from: pod
            key: app.kubernetes.io/name
            tag_name: service.name
          - from: pod
            key: app.kubernetes.io/name
            tag_name: k8s.app.name
          - from: pod
            key: k8s-app
            tag_name: service.name
          - from: pod
            key: app.kubernetes.io/instance
            tag_name: k8s.app.instance
          - from: pod
            key: app.kubernetes.io/version
            tag_name: service.version
          - from: pod
            key: app.kubernetes.io/component
            tag_name: k8s.app.component
          - from: pod
            key: app.kubernetes.io/part-of
            tag_name: k8s.app.part_of
          - from: pod
            key: app.kubernetes.io/managed-by
            tag_name: k8s.app.managed_by
          # Extract all labels
          # - tag_name: $$1
          #   key_regex: (.*)
          #   from: pod
          # annotations:
          # Extract all annotations
          # - tag_name: $$1
          #   key_regex: (.*)
          #   from: pod
          metadata:
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.node.name
          - k8s.namespace.name
          - k8s.pod.start_time
          - k8s.replicaset.name
          - k8s.replicaset.uid
          - k8s.daemonset.name
          - k8s.daemonset.uid
          - k8s.job.name
          - k8s.job.uid
          - k8s.cronjob.name
          - k8s.statefulset.name
          - k8s.statefulset.uid
          - container.image.name
          - container.image.tag
          - k8s.container.name
          - container.id
          - container.image.name
          - container.image.tag
          - container.id
          - k8s.cluster.uid
        filter:
          node_from_env_var: K8S_NODE_NAME
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
          - from: resource_attribute
            name: k8s.node.name
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
        - sources:
          - from: connection
      # transform:
      #   trace_statements:
      #   - context: resource
      #     statements:
      #     - set(attributes["k8s.cluster.name"], "${K8S_CLUSTER_NAME}")
      #     - set(attributes["cluster"], "${K8S_CLUSTER_NAME}")
      filter/healthCheck:
        error_mode: ignore
        traces:
          span:
          - 'attributes["http.target"] == "/health"'
      filter/openaisystem:
        error_mode: ignore
        traces:
          span:
          - 'attributes["gen_ai.system"] != "openai"'

    exporters:
      otlphttp/agenta:
        endpoint: "https://cloud.agenta.ai/api/otlp"
        headers:
          Authorization: "ApiKey ${env:AGENTA_API_KEY}" #

      otlphttp/langfuse:
        endpoint: "https://cloud.langfuse.com/api/public/otel" # EU data region
        headers:
          Authorization: "Basic ${env:LANGFUSE_AUTH}"
        protocol: http/protobuf

      otlphttp/langsmith:
        endpoint: https://api.smith.langchain.com/otel
        headers:
          x-api-key: ${env:LANGSMITH_API_KEY}
          Langsmith-Project: ${env:LANGSMITH_PROJECT}

    service:
      telemetry:
        logs:
          level: info
          encoding: json
        metrics:
          address: ${env:MY_POD_IP}:8888
      pipelines:
        logs: null
        metrics: null
        traces:
          receivers:
          - otlp
          processors:
          - memory_limiter
          - resourcedetection
          - k8sattributes
          - filter/healthCheck
          # - filter/openaisystem # only forward Spans which have a gen_ai.system attribute set to openai
          - batch
          exporters:
          - debug
          - otlphttp/langfuse

  ports:
    otlp:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      protocol: TCP
      appProtocol: grpc
    otlp-http:
      enabled: true
      containerPort: 4318
      servicePort: 4318
      protocol: TCP
    jaeger-compact:
      enabled: false
    jaeger-thrift:
      enabled: false
    jaeger-grpc:
      enabled: false
    zipkin:
      enabled: false
    metrics:
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP

  service:
    enabled: true

  serviceMonitor:
    enabled: true
    extraLabels:
      prometheus.io/operator: portefaix

  prometheusRule:
    enabled: true
    groups: []
    defaultRules:
      enabled: false
    extraLabels:
      prometheus.io/operator: portefaix
